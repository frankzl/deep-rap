{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Generating Word by Word\n",
    "\n",
    "This time we don't predict letter by letter, but word by word!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['as', 'real', 'as', 'it', 'seems', 'the', 'american', 'dream', '\\\\n', \"ain't\"], 'nothing'), (['real', 'as', 'it', 'seems', 'the', 'american', 'dream', '\\\\n', \"ain't\", 'nothing'], 'but'), (['as', 'it', 'seems', 'the', 'american', 'dream', '\\\\n', \"ain't\", 'nothing', 'but'], 'another'), (['it', 'seems', 'the', 'american', 'dream', '\\\\n', \"ain't\", 'nothing', 'but', 'another'], 'calculated'), (['seems', 'the', 'american', 'dream', '\\\\n', \"ain't\", 'nothing', 'but', 'another', 'calculated'], 'schemes')]\n"
     ]
    }
   ],
   "source": [
    "import tools.processing as pre\n",
    "import re\n",
    "\n",
    "# use less text for now to avoid memory error\n",
    "text = pre.get_text(\"data/cleaned-rap-lyrics/clean2_pac_.txt\")[:20002]\n",
    "\n",
    "vocab = pre.Vocabulary(text)\n",
    "\n",
    "TIMESTEPS = 10\n",
    "\n",
    "# double \\\\n to avoid null error in tensorboard projection\n",
    "text = text.replace(\"\\n\", \" \\\\n \")\n",
    "\n",
    "# remove extra spacing\n",
    "tokens = re.sub( \" +\", \" \", text).split(\" \")[:-1]\n",
    "\n",
    "str_data, str_labels = pre.create_data_label_pairs(tokens, TIMESTEPS)\n",
    "\n",
    "print( list( zip(str_data, str_labels) )[:5] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tools.training as tr\n",
    "\n",
    "class OneHotWordEncoder(tr.Encoder):\n",
    "    \"\"\"\n",
    "    Encodes sequences of words to sequences of 1-Hot Encoded vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name, word2index):\n",
    "        super(OneHotWordEncoder, self).__init__(name)\n",
    "        self.word2index = word2index\n",
    "        \n",
    "    def encode(self, sequences):\n",
    "        \"\"\"\n",
    "        Encodes our sequences of words to sequences of 1-Hots\n",
    "        \"\"\"\n",
    "        encoded_sequences = []\n",
    "        for seq in sequences:\n",
    "            \n",
    "            encoded = np.zeros( ( len(seq), len(self.word2index) ) )\n",
    "            \n",
    "            for idx, word in enumerate(seq):\n",
    "                encoded[idx][ self.word2index[word] ] = 1\n",
    "            \n",
    "            encoded_sequences.append(encoded)\n",
    "        \n",
    "        return np.array(encoded_sequences)\n",
    "    \n",
    "    def encode_raw(self, text):\n",
    "        \"\"\"\n",
    "        Encodes a text to sequences of 1-Hots (needed for sampling)\n",
    "        \"\"\"\n",
    "        text = text.replace(\"\\n\", \" \\\\n \")\n",
    "        text = text.replace(\" +\", \" \")\n",
    "        words = text.split(\" \")\n",
    "        encoded = np.zeros( ( len(words), len(self.word2index) ) )\n",
    "        \n",
    "        for idx, word in enumerate(words):\n",
    "            if word != \"\":\n",
    "                encoded[idx][ self.word2index[word] ] = 1\n",
    "        \n",
    "        return np.array( [encoded] )\n",
    "        \n",
    "    \n",
    "    def encode_labels(self, labels):\n",
    "        \"\"\"\n",
    "        Encodes the labels (sequences of one word)\n",
    "        \"\"\"\n",
    "        \n",
    "        encoded = []\n",
    "        \n",
    "        for label in labels:\n",
    "            one_hot_vec = np.zeros(len(self.word2index), dtype=int)\n",
    "            one_hot_vec[ self.word2index[label] ] = 1\n",
    "            encoded.append( one_hot_vec )\n",
    "            \n",
    "        return np.array(encoded)\n",
    "    \n",
    "class OneHotWordDecoder(tr.Decoder):\n",
    "    \"\"\"\n",
    "    Decodes a 1-Hot Encoded vector (prediction) to a word\n",
    "    \"\"\"\n",
    "    def __init__(self, name, index2word, temperature=0.5):\n",
    "        super(OneHotWordDecoder, self).__init__(name)\n",
    "        self.temperature = temperature\n",
    "        self.index2word = index2word \n",
    "        \n",
    "    def decode(self, predicted):\n",
    "        predicted = tr.sample_from_distribution(predicted, temperature=self.temperature)\n",
    "        return \" \" + self.index2word[ np.argmax(predicted) ].replace(\"\\\\n\",\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools.training as tr\n",
    "\n",
    "encoder = OneHotWordEncoder(\"1-Hot-Word-Encoding\", vocab.word2index)\n",
    "decoder = OneHotWordDecoder(\"1-Hot-Word-Decoding\", vocab.index2word, temperature=0.8)\n",
    "\n",
    "data = encoder.encode( str_data )\n",
    "labels = encoder.encode_labels( str_labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-61745f19b9e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_text\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mseed_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/coding/deep-rap/tools/training.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(trainable, train_data, train_labels, sampler, epochs, batch_size)\u001b[0m\n\u001b[1;32m     97\u001b[0m             tr_loss, tr_acc = session.run([trainable.loss, trainable.accuracy],\n\u001b[1;32m     98\u001b[0m                                            feed_dict={trainable.X: train_data,\n\u001b[0;32m---> 99\u001b[0;31m                                                       \u001b[0mtrainable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                                                      })\n\u001b[1;32m    101\u001b[0m             \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.envs/env36-ml/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.envs/env36-ml/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.envs/env36-ml/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.envs/env36-ml/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.envs/env36-ml/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.envs/env36-ml/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tools.architectures as nn\n",
    "\n",
    "HIDDEN_LAYER_SIZE = 512\n",
    "VOCAB_SIZE = vocab.get_size()\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "rnn = nn.SingleLayerRNN(name = \"basic\")\n",
    "rnn.build(HIDDEN_LAYER_SIZE, VOCAB_SIZE, TIMESTEPS, l2_reg=0.0)\n",
    "\n",
    "\n",
    "sampler = lambda trainable, seed_text: tr.sample( seed_text, trainable, encoder, decoder, length=20)\n",
    "\n",
    "tr.train_model(rnn, data, labels, sampler, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to sample on completely random sentences that we made up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler(rnn, \"while i go down the street \\n you was lookin' at me \\n is this even good \\n why\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alright, but far from good. Now increase the #TIMESTEPS and decrease temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 20\n",
    "\n",
    "str_data, str_labels = pre.create_data_label_pairs(tokens, TIMESTEPS)\n",
    "\n",
    "encoder = OneHotWordEncoder(\"1-Hot-Word-Encoding\", vocab.word2index)\n",
    "decoder = OneHotWordDecoder(\"1-Hot-Word-Decoding\", vocab.index2word, temperature=0.6)\n",
    "\n",
    "data = encoder.encode( str_data )\n",
    "labels = encoder.encode_labels( str_labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools.architectures as nn\n",
    "\n",
    "HIDDEN_LAYER_SIZE = 512\n",
    "VOCAB_SIZE = vocab.get_size()\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "rnn = nn.SingleLayerRNN(name = \"basic\")\n",
    "rnn.build(HIDDEN_LAYER_SIZE, VOCAB_SIZE, TIMESTEPS, l2_reg=0.0)\n",
    "\n",
    "\n",
    "sampler = lambda trainable, seed_text: tr.sample( seed_text, trainable, encoder, decoder, length=20)\n",
    "\n",
    "tr.train_model(rnn, data, labels, sampler, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler(rnn, \"while i go down the street \\n you was lookin' at me \\n is this even good or is it just bad \\n is this even good or is it just bad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learnings\n",
    "\n",
    "We are now not predicting letter by letter, but word by word. The model does not have to learn how to spell the words.\n",
    "After a few iterations we already get some sentences that make sense to a reader!\n",
    "\n",
    "Some of them seem like actual lyrics, e.g.\n",
    "\n",
    "**\"to get us locked up up up up up up in bust in and knowledge and lies and kick and me if don't you well\"**\n",
    "\n",
    "The now export the new encoder and decoder to our module **tools.training**\n",
    "\n",
    "Let's see how a multi layered LSTM performs in the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
