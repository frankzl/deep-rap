{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools.processing as pre\n",
    "text = pre.get_text(\"data/cleaned-rap-lyrics/cleaneminem.txt\")\n",
    "\n",
    "CONTRACTION_MAP = {\"'em\":\"them\",\"y'know\": \"you know\", \"'hem\": \"them\",                   \n",
    "                   \"c'mon\": \"come on\", \"'caine\": \"cocaine\",\n",
    "                   \"mo'\": \"my\", \"cha'\": \"ya\", \"'cha\": \"ya\",\n",
    "                   \"whaddya\": \"what do ya\", \"nuttin\": \"nothing\",\n",
    "                   \"lets\": \"let us\", \"let's\": \"let us\",\n",
    "                   \"f'real\": \"for real\",\n",
    "                   \"'til\": \"until\",\n",
    "                   \"i'ma\": \"i am going to\",\n",
    "                   \"ima\": \"i am going to\",\n",
    "                   \"'cross\": \"across\",\n",
    "                   \"imma\": \"i am going to\",\n",
    "                   \"tho'\": \"though\",\n",
    "                   \"st8\": \"straight\",\n",
    "                   \"til'\": \"until\",\n",
    "                   \"coz\": \"because\",\n",
    "                   \"coz'\": \"because\",\n",
    "                   \"cuz'\": \"because\",\n",
    "                   \"cuz\": \"because\",\n",
    "                   \"'im\": \"him\",\n",
    "                   \"'bout\": \"about\",\n",
    "                   \"'n\": \"and\",\n",
    "                   \"tha'\": \"the\",\n",
    "                   \"tu'\": \"to\",\n",
    "                   \"uz'\": \"daheck?\",\n",
    "                   \"yo'\": \"your\",\n",
    "                   \"witcha\": \"with ya\",\n",
    "                   \"wit'\": \"with\",\n",
    "                   \"whaddup\": \"what is up\",\n",
    "                   \"pro'lly\": \"probably\",\n",
    "                   \"'laxin\": \"relaxing\",\n",
    "                   \"tryna\": \"trying to\",\n",
    "                   \"'tack\": \"attack\",\n",
    "                   \"'head\": \"ahead\",\n",
    "                   \"lil'\": \"little\",\n",
    "                   \"getcha\": \"get ya\",\n",
    "                   \"wit'chu\": \"with you\",\n",
    "                   \"get'cha\": \"get ya\",\n",
    "                   \"sweatcha\": \"sweat ya\",\n",
    "                   \"e'ry\": \"every\",\n",
    "                   \"what'cha\": \"what ya\",\n",
    "                   \"hitcha\": \"hit ya\",\n",
    "                   \"hit'cha\": \"hit ya\",\n",
    "                   \"gov'na\": \"governor\",\n",
    "                   \"'fore\": \"before\",\n",
    "                   \"mill'\": \"million\",\n",
    "                   \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"can not\", \n",
    "                   \"can't've\": \"can not have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
    "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
    "                   \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",\n",
    "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                   \"this's\": \"this is\",\n",
    "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
    "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why'd\": \"why did\",\n",
    "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", \n",
    "                   \"gon'\": \"going to\",\n",
    "                   \" an'\": \"and\"\n",
    "                  } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Cleaning the data\n",
    "\n",
    "We need to correct typos such as \"imdamcwitdenastymouf\" = \"i am the mc with the nasty mouth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " send corrections to the typist ; you do not see me in the hood it is cause i am doing this man ; niggas i am still grinding yeah i still hearing those sirens ; i am still getting chased by those lights ; only the light 's lime and my mic 's on ; and my time is none because i am writing more ; i do not hear to meet a soul in this business ; i am here to eat speak until these ho 's feel this ; i is not gonna let you derail me man ; i got young kobe homey you gotta let go of obie ; because obie be back going nowhere man we got them craps going on ; and that rap going on soon as a nigga touch down back from town ; it is forever put that on the cheddar man ; but in the meantime it is jimmy iovine time ; chase cheese rhyme until my voice give out ; this is it my niggas this what we boast about ; now i am here so shut your motherfucking mouth and show me love bitch ; i just wanna love for the rest of my life i do not love you bitch ; i wanna hold you in the morn hold you in the night ; right\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "remove_lbreak = text.replace(\"\\n\", \" ; \")\n",
    "remove_space  = re.sub(\" +\", \" \", remove_lbreak)\n",
    "\n",
    "remove_space = remove_space.replace(\"'ll \", \" will \")\n",
    "remove_space = remove_space.replace(\"'ve \", \" have \")\n",
    "remove_space = remove_space.replace(\" u \", \" you \")\n",
    "remove_space = remove_space.replace(\" n \", \" and \")\n",
    "remove_space = remove_space.replace(\" 'n' \", \" and \")\n",
    "remove_space = remove_space.replace(\"'n \", \" and \")\n",
    "remove_space = remove_space.replace(\" de \", \" the \")\n",
    "remove_space = remove_space.replace(\" gon \", \" going to \")\n",
    "\n",
    "def expand_contractions(sentence, contraction_mapping):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "    return expanded_sentence\n",
    "\n",
    "\n",
    "expanded = expand_contractions(remove_space, CONTRACTION_MAP)\n",
    "expanded = expanded.replace(\"'s \", \" 's \")\n",
    "expanded = expanded.replace(\"s'\", \" 's \")\n",
    "expanded = expanded.replace(\"in'\", \"ing\")\n",
    "# expanded = expanded.replace(\"'ll\", \" will\")\n",
    "expanded = expanded.replace(\"mutha\", \"mother\")\n",
    "expanded = expanded.replace(\"en'\", \"ing\")\n",
    "expanded = expanded.replace(\"brotha\", \"brother\")\n",
    "expanded = expanded.replace(\"outta\", \"out of\")\n",
    "expanded = expanded.replace(\"2pac\", \"tupac\")\n",
    "expanded = expanded.replace(\" nite\", \" night\")\n",
    "print(expanded[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _dict = pre.Vocabulary(expanded)\n",
    "# t = \"\\n\".join(_dict._keys)\n",
    "# pre.write_text(\"data/my_dict.txt\", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Building a dictionary\n",
    "We load the found distinct words into a txt file *data/my_dict.txt*\n",
    "\n",
    "We slowly build up the dictionary from the given text to make sure we don't accept any typos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The final dictionary will be stored inside the file final_dict.txt**\n",
    "\n",
    "# Step 3: Correcting Typos using our new dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 7376),\n",
       " ('i', 6805),\n",
       " ('is', 4549),\n",
       " ('to', 3917),\n",
       " ('a', 3810),\n",
       " ('you', 3697),\n",
       " ('and', 3659),\n",
       " ('it', 3028),\n",
       " ('my', 2385),\n",
       " ('me', 2373),\n",
       " ('not', 2306),\n",
       " ('in', 2069),\n",
       " ('am', 1892),\n",
       " ('of', 1573),\n",
       " ('that', 1506),\n",
       " ('on', 1452),\n",
       " ('for', 1324),\n",
       " ('nigga', 1231),\n",
       " ('like', 1219),\n",
       " ('we', 1187)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import collections\n",
    "\n",
    "def tokens(text):\n",
    "    \"\"\"\n",
    "    Get all words from corpus\n",
    "    \"\"\"\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "wordlist = pre.get_text('data/ref_text3.txt')\n",
    "\n",
    "WORDS = tokens(wordlist) + [\";\"]\n",
    "WORD_COUNTS = collections.Counter(WORDS)\n",
    "\n",
    "WORD_COUNTS.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going\n",
      "total amount words: 194658\n"
     ]
    }
   ],
   "source": [
    "def edits0(word):\n",
    "    \"\"\"\n",
    "    Return all strings that are zero edits away (i.e. the word itself).\n",
    "    \"\"\"\n",
    "    return{word}\n",
    "\n",
    "def edits1(word):\n",
    "    \"\"\"\n",
    "    Return all strings that are one edits away.\n",
    "    \"\"\"\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    def splits(word):\n",
    "        \"\"\"\n",
    "        return a list of all possible pairs\n",
    "        that the input word is made of\n",
    "        \"\"\"\n",
    "        return [(word[:i], word[i:]) for i in range(len(word)+1)]\n",
    "    pairs = splits(word)\n",
    "    deletes = [a+b[1:] for (a,b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a,b) in pairs if len(b) >1]\n",
    "    replaces = [a+c+b[1:] for (a,b) in pairs for c in alphabet if b]\n",
    "    inserts = [a+c+b for (a,b) in pairs for c in alphabet]\n",
    "    return(set(deletes + transposes + replaces + inserts))\n",
    "\n",
    "def edits2(word):\n",
    "    \"\"\"\n",
    "    return all strings that are two edits away.\n",
    "    \"\"\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}\n",
    "\n",
    "def known(words):\n",
    "    return {w for w in words if w in WORD_COUNTS}\n",
    "\n",
    "unk = []\n",
    "def correct(word):\n",
    "    candidates = (known(edits0(word)) or\n",
    "                 known(edits1(word)) or\n",
    "                 known(edits2(word)) or\n",
    "                 [word])\n",
    "    cand = max(candidates, key=WORD_COUNTS.get)\n",
    "    if(candidates == known(edits2(word))):\n",
    "        if(cand != word):\n",
    "            unk.append(word)\n",
    "            # print(f\"{cand} - {word}\")\n",
    "    # return max(candidates, key=WORD_COUNTS.get)\n",
    "    return cand\n",
    "\n",
    "def correct_text(text):\n",
    "    processed = text.replace('\\n', ' \\n ')\n",
    "    corrected = [ correct(word) for word in processed.split(\" \") ]\n",
    "    \n",
    "    return \" \".join(corrected)\n",
    "\n",
    "_temp = text.replace('\\n', ' \\n ')\n",
    "words_length = len(_temp.split(\" \"))\n",
    "print (correct(\"goin\"))\n",
    "print (f'total amount words: {words_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" send corrections to the typist ; you do not see me in the hood it is cause i am doing this man ; niggas i am still grinding yeah i still hearing those sirens ; i am still getting chased by those lights ; only the light 's lime and my mic 's on ; and my time is none because i am writing more ; i do \""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-25761e12d5a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorrected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-de98908a8e69>\u001b[0m in \u001b[0;36mcorrect_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcorrect_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' \\n '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mcorrected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-de98908a8e69>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcorrect_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' \\n '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mcorrected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-de98908a8e69>\u001b[0m in \u001b[0;36mcorrect\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     39\u001b[0m                  [word])\n\u001b[1;32m     40\u001b[0m     \u001b[0mcand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWORD_COUNTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medits2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcand\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0munk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-de98908a8e69>\u001b[0m in \u001b[0;36medits2\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall\u001b[0m \u001b[0mstrings\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mare\u001b[0m \u001b[0mtwo\u001b[0m \u001b[0medits\u001b[0m \u001b[0maway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \"\"\"\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0me2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0medits1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0medits1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-de98908a8e69>\u001b[0m in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall\u001b[0m \u001b[0mstrings\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mare\u001b[0m \u001b[0mtwo\u001b[0m \u001b[0medits\u001b[0m \u001b[0maway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \"\"\"\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0me2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0medits1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0medits1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corrected = correct_text(expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unknown words: 499\n"
     ]
    }
   ],
   "source": [
    "print( f\"number of unknown words: {len(unk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "befriend\n",
      "heeeey\n",
      "imma\n",
      "beretta\n",
      "dismiss\n",
      "10deep\n",
      "growth\n",
      "treatin\n",
      "sweatshirt\n",
      "lowest\n",
      "experimented\n",
      "silently\n",
      "conceils\n",
      "cleaners\n",
      "whiskey\n",
      "libbing\n",
      "travie\n",
      "timid\n",
      "grillin\n",
      "prada\n",
      "tiva\n",
      "hadhad\n",
      "zombie\n",
      "however\n",
      "stealth\n",
      "loins\n",
      "squirt\n",
      "blackest\n",
      "blonde\n",
      "tinted\n",
      "sanity\n",
      "punching\n",
      "metaphoric\n",
      "cocoon\n",
      "groupies\n",
      "shoewear\n",
      "rrright\n",
      "heyhey\n",
      "playful\n",
      "absurd\n",
      "shittin\n",
      "covered\n",
      "shrooms\n",
      "artsy\n",
      "strolling\n",
      "opened\n",
      "witya\n",
      "'88\n",
      "ohio\n",
      "unruly\n",
      "profession\n",
      "yeezy\n",
      "chapter\n",
      "doubting\n",
      "ignorance\n",
      "monica\n",
      "glimmer\n",
      "unified\n",
      "cliche\n",
      "krusty\n",
      "incomplete\n",
      "redrum\n",
      "outkast\n",
      "gooder\n",
      "safari\n",
      "carcuses\n",
      "blanco\n",
      "trippy\n",
      "almighty\n",
      "applause\n",
      "outer\n",
      "23\n",
      "snorted\n",
      "amidst\n",
      "reclining\n",
      "pursuit\n",
      "escaping\n",
      "trynna\n",
      "whaddup\n",
      "wearer\n",
      "goooooo\n",
      "cravens\n",
      "electro\n",
      "splash\n",
      "pressin\n",
      "kusher\n",
      "commandment\n",
      "aspects\n",
      "eggs\n",
      "shinnin\n",
      "emotional\n",
      "shagged\n",
      "carriage\n",
      "pro'lly\n",
      "explicit\n",
      "22\n",
      "wolves\n",
      "starvin\n",
      "mamama\n",
      "paula\n",
      "fancy\n",
      "hovered\n",
      "calmin\n",
      "25th\n",
      "converse\n",
      "wildand\n",
      "scalp\n",
      "vividly\n",
      "30\n",
      "googled\n",
      "stanton\n",
      "slumber\n",
      "somthin\n",
      "strutting\n",
      "coastses\n",
      "navalet\n",
      "hopeful\n",
      "legion\n",
      "monsoon\n",
      "99\n",
      "wonderous\n",
      "wildest\n",
      "posted\n",
      "hyyerr\n",
      "'laxin\n",
      "myth\n",
      "powpow\n",
      "rapidly\n",
      "fet'\n",
      "edits\n",
      "stamina\n",
      "harmful\n",
      "hipster\n",
      "slowmo\n",
      "ahoy\n",
      "parallel\n",
      "seemingly\n",
      "imeem\n",
      "awww\n",
      "tangle\n",
      "credits\n",
      "maui\n",
      "demented\n",
      "drownin\n",
      "stalling\n",
      "dopeness\n",
      "recent\n",
      "tryna\n",
      "delayin\n",
      "carbide\n",
      "comic\n",
      "igloo\n",
      "throwed\n",
      "embody\n",
      "50\n",
      "cutlass\n",
      "gargle\n",
      "heeeeey\n",
      "yuppers\n",
      "favorites\n",
      "illuminate\n",
      "shrink\n",
      "shocking\n",
      "zonin\n",
      "lion\n",
      "mickey\n",
      "splittin\n",
      "device\n",
      "sleeper\n",
      "dooder\n",
      "possum\n",
      "blooded\n",
      "rented\n",
      "ima\n",
      "spleen\n",
      "xannies\n",
      "untamed\n",
      "yammin\n",
      "southside\n",
      "cooler\n",
      "recessions\n",
      "viennas\n",
      "ayyy\n",
      "vicariously\n",
      "provide\n",
      "godly\n",
      "twiddling\n",
      "grizzle\n",
      "jibber\n",
      "92\n",
      "wolf\n",
      "patton\n",
      "crusade\n",
      "horizon\n",
      "outtie\n",
      "stripper\n",
      "whiskee\n",
      "ixnay\n",
      "disperse\n",
      "imagination\n",
      "hurdles\n",
      "mamoon\n",
      "commit\n",
      "24\n",
      "osa\n",
      "gaga\n",
      "'08\n",
      "loony\n",
      "golet\n",
      "profound\n",
      "lantern\n",
      "greeting\n",
      "workout\n",
      "litmus\n",
      "bathing\n",
      "hunger\n",
      "edamame\n",
      "privates\n",
      "towlet\n",
      "troubled\n",
      "piercing\n",
      "liiiiike\n",
      "ooooooh\n",
      "travis\n",
      "nellie\n",
      "likely\n",
      "acid\n",
      "fatigue\n",
      "neva\n",
      "eerie\n",
      "smirk\n",
      "zoning\n",
      "halfway\n",
      "diff\n",
      "creepers\n",
      "cudders\n",
      "secretly\n",
      "sailor\n",
      "wizard\n",
      "uneasy\n",
      "optimos\n",
      "guntoter\n",
      "weekly\n",
      "mainframe\n",
      "smoothie\n",
      "mgmt\n",
      "wowie\n",
      "cudder\n",
      "stunting\n",
      "cancun\n",
      "skippin\n",
      "ohohoh\n",
      "looney\n",
      "foreigner\n",
      "murmurs\n",
      "rainstorms\n",
      "swivel\n",
      "toledo\n",
      "urkel\n",
      "imagined\n",
      "prejudice\n",
      "skating\n",
      "literal\n",
      "isoh\n",
      "6th\n",
      "pivot\n",
      "citrus\n",
      "honestly\n",
      "strippers\n",
      "'cross\n",
      "mackin\n",
      "'fro\n",
      "providing\n",
      "navy\n",
      "choo\n",
      "frosty\n",
      "snapback\n",
      "feasting\n",
      "ritz\n",
      "turtle\n",
      "ballsac\n",
      "hooves\n",
      "befo'\n",
      "kaboom\n",
      "88\n",
      "bzzzz\n",
      "outlives\n",
      "laughed\n",
      "dimples\n",
      "french\n"
     ]
    }
   ],
   "source": [
    "for i in set(unk):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add all corrected unknowns to the reference text and rerun the correction process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools.processing as pre\n",
    "\n",
    "def recorrect(word):\n",
    "    candidates = (known(edits0(word)) or\n",
    "                 known(edits1(word)) or\n",
    "                 known(edits2(word)) or\n",
    "                 [word])\n",
    "    return max(candidates, key=WORD_COUNTS.get)\n",
    "\n",
    "def recorrect_text(text):\n",
    "    processed = text.replace('\\n', ' \\n ')\n",
    "    corrected = [ recorrect(word) for word in processed.split(\" \") ]\n",
    "    \n",
    "    return \" \".join(corrected)\n",
    "\n",
    "wordlist = pre.get_text('data/ref_text3.txt')\n",
    "\n",
    "corrected = recorrect_text(expanded)\n",
    "\n",
    "# ref = pre.get_text(\"data/ref_text.txt\")\n",
    "# vocab = pre.Vocabulary(ref)\n",
    "\n",
    "# len(vocab._keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jones ; we born to die for real ; so i get high just like i am born to fly to the moon ; i am in the court with marijuana eyes sorry judge ; we hit the clubs until the morning rise cause of last night ; cause day and night day and night ; we run the streets hope i do not pay that price pray for me ; they got the nerve to tell me press my brakes what ; and if i do then how will rent get paid paid ; so now i am thinking that what is your thought ; two hundred stacks what would it bring me back thi'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['88', '23', '23', '08', '09', '50', '88', '99', '30', '23', '23', '88', '23', '50', '22', '24', '92', '99', '22', '88']\n"
     ]
    }
   ],
   "source": [
    "words = corrected.split(\" \")\n",
    "numbers_str = filter( lambda word: word.isnumeric(), words )\n",
    "print(list(numbers_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['88', '23', '23', '08', '09', '50', '88', '99', '6th', '30', '23', '23', '25th', '88', '23', '50', '22', '24', '92', '99', '22', '88']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r\"\\w*[0-9]+\\w*\", corrected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected = corrected.replace(\"18th\", \"eighteenth\")\n",
    "corrected = corrected.replace(\"56ths\", \"eighteenth\")\n",
    "corrected = corrected.replace(\"110th\", \"hundred and tenth\")\n",
    "corrected = corrected.replace(\"25th\", \"twenty fifth\")\n",
    "corrected = corrected.replace(\"5th\", \"fifth\")\n",
    "corrected = corrected.replace(\"4th\", \"fourth\")\n",
    "corrected = corrected.replace(\"6th\", \"sixth\")\n",
    "corrected = corrected.replace(\"3d\", \"three dee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fifty five nine one one\n"
     ]
    }
   ],
   "source": [
    "import inflect\n",
    "\n",
    "def number_to_word(number_str):\n",
    "    \n",
    "    p = inflect.engine()\n",
    "    \n",
    "    if   (len(number_str) <= 2):\n",
    "        number = p.number_to_words(int(number_str))\n",
    "        if(number == \"zero\"):\n",
    "            return \"ou\"\n",
    "        else: \n",
    "            return number.replace(\"-\",\" \")\n",
    "    \n",
    "    elif (len(number_str) == 4):\n",
    "        digit_1 = int( number_str[:2] )\n",
    "        digit_2 = int( number_str[2:] )\n",
    "        number = p.number_to_words(digit_1) + \" \" + p.number_to_words(digit_2)\n",
    "        if(int(number_str[2:]) < 10):\n",
    "            return p.number_to_words(int(number_str)).replace(\",\", \"\")\n",
    "        return number.replace(\"-\", \" \")\n",
    "        \n",
    "    else:\n",
    "        val = \" \".join( [p.number_to_words(int(digit)) for digit in number_str] )\n",
    "        return val.replace(\"zero\", \"ou\")\n",
    "\n",
    "print( number_to_word(\"55\"), number_to_word(\"911\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'92': 'ninety two',\n",
       " '08': 'eight',\n",
       " '99': 'ninety nine',\n",
       " '30': 'thirty',\n",
       " '23': 'twenty three',\n",
       " '22': 'twenty two',\n",
       " '88': 'eighty eight',\n",
       " '50': 'fifty',\n",
       " '24': 'twenty four',\n",
       " '09': 'nine'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_number_mapping( text ):\n",
    "    words = text.split(\" \")\n",
    "    numbers_str = filter( lambda word: word.isnumeric(), words )\n",
    "    \n",
    "    mapping = {}\n",
    "    for number in set(numbers_str):\n",
    "        mapping[number] = number_to_word(number)\n",
    "    return mapping\n",
    "\n",
    "encoding = get_number_mapping(corrected)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jones ; we born to die for real ; so i get high just like i am born to fly to the moon ; i am in the court with marijuana eyes sorry judge ; we hit the clubs until the morning rise cause of last night ; cause day and night day and night ; we run the streets hope i do not pay that price pray for me ; they got the nerve to tell me press my brakes what ; and if i do then how will rent get paid paid ; so now i am thinking that what is your thought ; two hundred stacks what would it bring me back thi\n"
     ]
    }
   ],
   "source": [
    "def encode_numbers(text, encoding):\n",
    "    encoded = text\n",
    "    \n",
    "    items = list( encoding.items() )\n",
    "    items.sort(key=lambda item: len( item[0]), reverse=True)\n",
    "    \n",
    "    for number, substitute in items:\n",
    "        encoded = encoded.replace(number, substitute)\n",
    "    return encoded\n",
    "\n",
    "number_encoded = encode_numbers(corrected, encoding)\n",
    "               \n",
    "print( number_encoded[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre.write_text(\"data/prepped/cleankid_cudi.txt\", number_encoded )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
