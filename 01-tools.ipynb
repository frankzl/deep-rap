{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Introducing the tools\n",
    "\n",
    "We want to outsource some of the functions from the previous notebook to external modules. The train methods for example can be reused, as it is the same for every model\n",
    "\n",
    "At the moment it looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def train(self, train_data, train_labels, alphabet, epochs=20, batch_size=128):\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        \n",
    "        self.session = tf.Session()\n",
    "        session = self.session\n",
    "        \n",
    "        with session.as_default():\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            tr_loss, tr_acc = session.run([self.loss, self.accuracy],\n",
    "                                          feed_dict={self.X: train_data,\n",
    "                                                     self.Y: train_labels})\n",
    "            train_losses.append(tr_loss)\n",
    "            train_accs.append(tr_acc)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                \n",
    "                if(epoch + 1) % 1 == 0:\n",
    "                    print(f\"\\n\\nEpoch {epoch + 1}/{epochs}\")\n",
    "                    print(f\"Loss:    \\t {tr_loss}\")\n",
    "                    print(f\"Accuracy:\\t {tr_acc}\")\n",
    "                \n",
    "                for batch_ixs in batch_data(len(train_data), batch_size):\n",
    "                    _ = session.run(self.train_step,\n",
    "                                   feed_dict={\n",
    "                                       self.X: train_data[batch_ixs],\n",
    "                                       self.Y: train_labels[batch_ixs],\n",
    "                                   })\n",
    "                tr_loss, tr_acc = session.run([self.loss, self.accuracy],\n",
    "                                               feed_dict={self.X: train_data,\n",
    "                                                          self.Y: train_labels\n",
    "                                                         })\n",
    "                train_losses.append(tr_loss)\n",
    "                train_accs.append(tr_acc)\n",
    "                \n",
    "                #get on of training set as seed\n",
    "                seed = train_data[:1:]\n",
    "        \n",
    "                #to print the seed 40 characters\n",
    "                seed_chars = ''\n",
    "                for each in seed[0]:\n",
    "                    seed_chars += alphabet._keys[np.where(each == max(each))[0][0]]\n",
    "                print (\"Seed:\" + seed_chars)\n",
    "        \n",
    "                #predict next 500 characters\n",
    "                for i in range(500):\n",
    "                    if i > 0:\n",
    "                        remove_fist_char = seed[:,1:,:]\n",
    "                        seed = np.append(remove_fist_char, np.reshape(probabilities, [1, 1, self.vocab_size]), axis=1)\n",
    "                        \n",
    "                    predicted = session.run([self.final_output], feed_dict = {self.X:seed})\n",
    "                    predicted = np.asarray(predicted[0]).astype('float64')[0]\n",
    "                    probabilities = sample(predicted)\n",
    "                    predicted_chars = alphabet._keys[np.argmax(probabilities)]\n",
    "                    seed_chars += predicted_chars\n",
    "                print ('Result:'+ seed_chars)\n",
    "        \n",
    "        self.hist = {\n",
    "            'train_losses': np.array(train_losses),\n",
    "            'train_accuracy': np.array(train_accs)\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainable Object\n",
    "The Trainable contains all the attributes needed for training. All our implemeted models from now on should contain these attributes\n",
    "\n",
    "We can now generalize our train function as seen in our module ***tools.architectures***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Trainable:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        \n",
    "        self.final_output = None\n",
    "        self.loss = None\n",
    "        self.accuracy = None\n",
    "        self.train_step = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling function\n",
    "We need a proper sampling function that:\n",
    "1. feeds a seed into the model\n",
    "2. translate output of model into readable text\n",
    "\n",
    "At the moment, we have\n",
    "\n",
    "```python\n",
    "#get on of training set as seed\n",
    "seed = train_data[:1:]\n",
    "\n",
    "#to print the seed 40 characters\n",
    "seed_chars = ''\n",
    "for each in seed[0]:\n",
    "    seed_chars += alphabet._keys[np.where(each == max(each))[0][0]]\n",
    "    print (\"Seed:\" + seed_chars)\n",
    "    \n",
    "#predict next 500 characters\n",
    "for i in range(500):\n",
    "    if i > 0:\n",
    "        remove_fist_char = seed[:,1:,:]\n",
    "        seed = np.append(remove_fist_char, np.reshape(probabilities, [1, 1, self.vocab_size]), axis=1)\n",
    "        \n",
    "    predicted = session.run([self.final_output], feed_dict = {self.X:seed})\n",
    "    predicted = np.asarray(predicted[0]).astype('float64')[0]\n",
    "    probabilities = sample(predicted)\n",
    "    predicted_chars = alphabet._keys[np.argmax(probabilities)]\n",
    "    seed_chars += predicted_chars\n",
    "print ('Result:'+ seed_chars)\n",
    "```\n",
    "\n",
    "What we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample( seed_text, trainable, encoder, decoder, length=40 ):\n",
    "    \n",
    "    \"\"\" prints the sampled string\n",
    "    \n",
    "    seed_text: string of the seed, must have minimum length of our timestep size\n",
    "    \n",
    "    trainable: object model to sample from\n",
    "    \n",
    "    encoder: encoder object to encode the seed_text\n",
    "    \n",
    "    decoder: decoder object to decode the output from the trainable\n",
    "    \n",
    "    length: how many symbols we want to sample\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    seed = encoder.encode( [seed_text] )\n",
    "\n",
    "    #to print the seed characters\n",
    "    seed_chars = seed_text\n",
    "    print( \"------Sampling----------\" )\n",
    "    print( f\"seed: \\t{seed_text}\" )\n",
    "        \n",
    "    #predict next symbols\n",
    "    for i in range(length):\n",
    "        if i > 0:\n",
    "            seed = encoder.encode( [seed_chars] )\n",
    "            # Take only the last required symbols\n",
    "            seed = seed[:,-1*trainable.time_steps:,:]\n",
    "            \n",
    "            # remove_fist_char = seed[:,1:,:]\n",
    "            # seed = np.append(remove_fist_char, np.reshape(probabilities, [1, 1, trainable.vocab_size]), axis=1)\n",
    "            \n",
    "        predicted = trainable.session.run([trainable.final_output], feed_dict = {trainable.X:seed})\n",
    "        predicted = np.asarray(predicted[0]).astype('float64')[0]\n",
    "        \n",
    "        predicted_symbol = decoder.decode( predicted )\n",
    "        seed_chars += predicted_symbol\n",
    "    print ('result:'+ seed_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def encode(self, seed_chars):\n",
    "        pass\n",
    "    \n",
    "class Decoder:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def decode(self, predicted):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now whenever we want to sample during our training process, we pass a sampling function as an argument to our train method.\n",
    "\n",
    "A possible function call could look like:\n",
    "```python\n",
    "encoder = Encoder(\"encoder\")\n",
    "decoder = Decoder(\"decoder\")\n",
    "sampler = lambda trainable, seed_text: sample( seed_text, trainable, encoder, decoder, length=40)\n",
    "\n",
    "train( model, train_data, train_labels, sampler, epochs, batch_size, temperature )\n",
    "\n",
    "```\n",
    "\n",
    "Let's put these functions together inside the train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainable, train_data, train_labels, sampler, epochs=20, batch_size=128):\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    \n",
    "    trainable.session = tf.Session()\n",
    "    session = trainable.session\n",
    "    \n",
    "    with session.as_default():\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        tr_loss, tr_acc = session.run([trainable.loss, trainable.accuracy],\n",
    "                                      feed_dict={trainable.X: train_data,\n",
    "                                                 trainable.Y: train_labels})\n",
    "        train_losses.append(tr_loss)\n",
    "        train_accs.append(tr_acc)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "             \n",
    "            for batch_ixs in batch_data(len(train_data), batch_size):\n",
    "                _ = session.run(trainable.train_step,\n",
    "                               feed_dict={\n",
    "                                   trainable.X: train_data[batch_ixs],\n",
    "                                   trainable.Y: train_labels[batch_ixs],\n",
    "                               })\n",
    "            tr_loss, tr_acc = session.run([trainable.loss, trainable.accuracy],\n",
    "                                           feed_dict={trainable.X: train_data,\n",
    "                                                      trainable.Y: train_labels\n",
    "                                                     })\n",
    "            train_losses.append(tr_loss)\n",
    "            train_accs.append(tr_acc)\n",
    "            \n",
    "            if(epoch + 1) % 1 == 0:\n",
    "                print(f\"\\n\\nEpoch {epoch + 1}/{epochs}\")\n",
    "                print(f\"Loss:    \\t {tr_loss}\")\n",
    "                print(f\"Accuracy:\\t {tr_acc}\")\n",
    "            \n",
    "            \n",
    "            #get on of training set as seed\n",
    "            # seed_text = train_data[0]\n",
    "            # seed_text = train_data[0]\n",
    "            seed_text = \"as real as it seems the american dream\\nain't nothing but another calculated schemes\\nto get us locked up\"\n",
    "            \n",
    "            sampler(trainable, seed_text[:trainable.time_steps])\n",
    "            \n",
    "    \n",
    "    trainable.hist = {\n",
    "        'train_losses': np.array(train_losses),\n",
    "        'train_accuracy': np.array(train_accs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Single_Layer_LSTM_Classifier(Trainable):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "        \n",
    "    def build(self, hidden_layer_size, vocab_size, time_steps, l2_reg=0.0):\n",
    "        self.time_steps = time_steps\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape=[None, time_steps, vocab_size], name=\"data\")\n",
    "        self.Y = tf.placeholder(tf.int16, shape=[None, vocab_size], name=\"labels\")\n",
    "        \n",
    "        _X = tf.transpose(self.X, [1, 0, 2])\n",
    "        _X = tf.reshape(_X, [-1, vocab_size])\n",
    "        _X = tf.split(_X, time_steps, 0)\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            # 1x RNN LSTM Cell\n",
    "            self.rnn_cell   = tf.nn.rnn_cell.LSTMCell(hidden_layer_size)\n",
    "            \n",
    "            self.outputs, _ = tf.contrib.rnn.static_rnn(self.rnn_cell, _X, dtype=tf.float32)\n",
    "            \n",
    "            # 1x linear output layer\n",
    "            W_out = tf.Variable(tf.truncated_normal([hidden_layer_size, vocab_size], \n",
    "                                                 mean=0, stddev=.01))\n",
    "            b_out = tf.Variable(tf.truncated_normal([vocab_size],\n",
    "                                                mean=0, stddev=.01))\n",
    "            self.weights.append(W_out)\n",
    "            self.biases.append(b_out)\n",
    "            \n",
    "            self.last_rnn_output = self.outputs[-1]\n",
    "            self.final_output    = self.last_rnn_output @ W_out + b_out\n",
    "            \n",
    "            # softmax cross entropy as our loss function (between 36 classes)\n",
    "            self.softmax = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.final_output,\n",
    "                                                                labels=self.Y)\n",
    "            self.cross_entropy_loss = tf.reduce_mean(self.softmax)\n",
    "            \n",
    "            self.loss = self.cross_entropy_loss\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer()\n",
    "            self.train_step= self.optimizer.minimize(self.loss)\n",
    "            \n",
    "            self.correct_prediction = tf.equal(tf.argmax(self.Y,1), tf.argmax(self.final_output, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))*100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(num_data, batch_size):\n",
    "    \"\"\" Yield batches with indices until epoch is over.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_data: int\n",
    "        The number of samples in the dataset.\n",
    "    batch_size: int\n",
    "        The batch size used using training.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    batch_ixs: np.array of ints with shape [batch_size,]\n",
    "        Yields arrays of indices of size of the batch size until the epoch is over.\n",
    "    \"\"\"\n",
    "    \n",
    "    # data_ixs = np.random.permutation(np.arange(num_data))\n",
    "    data_ixs = np.arange(num_data)\n",
    "    ix = 0\n",
    "    while ix + batch_size < num_data:\n",
    "        batch_ixs = data_ixs[ix:ix+batch_size]\n",
    "        ix += batch_size\n",
    "        yield batch_ixs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools.processing as pre\n",
    "import tools.architectures as nn\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pre.get_text(\"data/cleaned-rap-lyrics/clean2_pac_.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = pre.Alphabet(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_distribution(predicted, temperature=0.9):\n",
    "    '''\n",
    "     helper function to sample an index from a probability array\n",
    "     our model will output scores for each class\n",
    "     we normalize those outputs and create a probability distribution out of them to sample from\n",
    "    '''\n",
    "    exp_predicted = np.exp(predicted/temperature)\n",
    "    predicted = exp_predicted / np.sum(exp_predicted)\n",
    "    probabilities = np.random.multinomial(1, predicted, 1)\n",
    "    return probabilities\n",
    "\n",
    "class OneHotEncoder(Encoder):\n",
    "    \"\"\"\n",
    "    Encodes sequences of words to sequences of 1-Hot Encoded vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name, word2index):\n",
    "        super(OneHotEncoder, self).__init__(name)\n",
    "        self.word2index = word2index\n",
    "        \n",
    "    def encode(self, sequences):\n",
    "        encoded_sequences = []\n",
    "        for seq in sequences:\n",
    "            encoded = np.zeros( ( len(seq), len(self.word2index) ) )\n",
    "            \n",
    "            for idx, symbol in enumerate(seq):\n",
    "                encoded[idx][ self.word2index[symbol] ] = 1\n",
    "            \n",
    "            encoded_sequences.append(encoded)\n",
    "        \n",
    "        return np.array(encoded_sequences)\n",
    "    \n",
    "class OneHotDecoder(Decoder):\n",
    "    \"\"\"\n",
    "    Decodes a 1-Hot Encoded vector (prediction) to a word\n",
    "    \"\"\"\n",
    "    def __init__(self, name, index2word, temperature=0.5):\n",
    "        super(OneHotDecoder, self).__init__(name)\n",
    "        self.temperature = temperature\n",
    "        self.index2word = index2word \n",
    "        \n",
    "    def decode(self, predicted):\n",
    "        predicted = sample_from_distribution(predicted, temperature=self.temperature)\n",
    "        return self.index2word[ np.argmax(predicted) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1/20\n",
      "Loss:    \t 2.857357978820801\n",
      "Accuracy:\t 17.976741790771484\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems b c t  ae sh noa '  hee \n",
      "  y t nhg een  e ioaiteue  h hee at c \n",
      "e  ee\n",
      "a th ss es teth t 'd i t ch  n\n",
      "\n",
      "\n",
      "Epoch 2/20\n",
      "Loss:    \t 2.5788729190826416\n",
      "Accuracy:\t 27.000125885009766\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems aat he tee baat mom me t lan the ran sin  a y tout the wot  ior thin the\n",
      "m eut ee ine  h the he me w\n",
      "\n",
      "\n",
      "Epoch 3/20\n",
      "Loss:    \t 2.445265769958496\n",
      "Accuracy:\t 29.391019821166992\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems ne ble then the the the bhe the yome theul md she teun  rock an iri the che than pot sot un p toa ho\n",
      "\n",
      "\n",
      "Epoch 4/20\n",
      "Loss:    \t 2.3677423000335693\n",
      "Accuracy:\t 30.39139747619629\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems sou in' not i gocgey ca the but the mace her shen' ho keop krand the me wit me the thet\n",
      "in bot the t\n",
      "\n",
      "\n",
      "Epoch 5/20\n",
      "Loss:    \t 2.3116695880889893\n",
      "Accuracy:\t 31.221708297729492\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems the thes be a goukin the bus the ba't cat sif whove thot at rous me i cath an in' geand bam the home\n",
      "\n",
      "\n",
      "Epoch 6/20\n",
      "Loss:    \t 2.2674758434295654\n",
      "Accuracy:\t 32.45967483520508\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems so the souss nous an it inn the sugpe cat cat doy i'f i gou ghe dout is le thet\n",
      "ind than the the the\n",
      "\n",
      "\n",
      "Epoch 7/20\n",
      "Loss:    \t 2.2294533252716064\n",
      "Accuracy:\t 33.655120849609375\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems the whet and at an the the the mes berther the micken 't you they it in than hous the baunt tha thet\n",
      "\n",
      "\n",
      "Epoch 8/20\n",
      "Loss:    \t 2.1948790550231934\n",
      "Accuracy:\t 34.19282150268555\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems in they buck gome the mut he cant a stolk losthe gat in the don than ihe walk yor\n",
      "ghe me soa here th\n",
      "\n",
      "\n",
      "Epoch 9/20\n",
      "Loss:    \t 2.161717653274536\n",
      "Accuracy:\t 34.95811080932617\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems now stout a mocker thenger a mot be keat ti sous dou's the back pouth the gat he you hou o me king\n",
      "t\n",
      "\n",
      "\n",
      "Epoch 10/20\n",
      "Loss:    \t 2.132140874862671\n",
      "Accuracy:\t 36.306114196777344\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems not the ghat on the get me then souce mathe they are the the get the the bean the cat leaze the ding\n",
      "\n",
      "\n",
      "Epoch 11/20\n",
      "Loss:    \t 2.1041765213012695\n",
      "Accuracy:\t 37.459049224853516\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems cout they bed a me and the mecger whing the beas the cher gat and soun the the sond i the the canat \n",
      "\n",
      "\n",
      "Epoch 12/20\n",
      "Loss:    \t 2.0778796672821045\n",
      "Accuracy:\t 38.28435516357422\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems got they in th pastin' seoke a got i dove butin' i the cauth the wang and the mes eat why a what my \n",
      "\n",
      "\n",
      "Epoch 13/20\n",
      "Loss:    \t 2.053586006164551\n",
      "Accuracy:\t 38.8870849609375\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems in the hay and and buther cack on i thing the my peat wing the counj the bus at be and cacke\n",
      "the pac\n",
      "\n",
      "\n",
      "Epoch 14/20\n",
      "Loss:    \t 2.0309391021728516\n",
      "Accuracy:\t 39.33475112915039\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems the wher they i wank ould the bave stor hes herlon and the the bother they fuck\n",
      "you dall the bust ro\n",
      "\n",
      "\n",
      "Epoch 15/20\n",
      "Loss:    \t 2.0113236904144287\n",
      "Accuracy:\t 39.82993698120117\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems i wann in the wing you santin' the gean ar get and an the here that cheld the wang kin' seat the get\n",
      "\n",
      "\n",
      "Epoch 16/20\n",
      "Loss:    \t 1.9904654026031494\n",
      "Accuracy:\t 39.9949951171875\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems be the brope i andie blath the got the wingat cough the when on the ged the bling the her be tougg m\n",
      "\n",
      "\n",
      "Epoch 17/20\n",
      "Loss:    \t 1.9704172611236572\n",
      "Accuracy:\t 40.46767807006836\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems miget and of the bant\n",
      "the ther was the bucke the wand an the got i bet so got when i gat the the get\n",
      "\n",
      "\n",
      "Epoch 18/20\n",
      "Loss:    \t 1.9517264366149902\n",
      "Accuracy:\t 40.84281539916992\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems bast ther get the higgat whan i'm cale the all be the stoll a beat doon that she bast i cand i light\n",
      "\n",
      "\n",
      "Epoch 19/20\n",
      "Loss:    \t 1.933908224105835\n",
      "Accuracy:\t 41.20545196533203\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems bother shes wante the santin chan when and cought wis the black a dount when the the bust\n",
      "the when t\n",
      "\n",
      "\n",
      "Epoch 20/20\n",
      "Loss:    \t 1.9169390201568604\n",
      "Accuracy:\t 41.60060119628906\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems the gith and speath and dough\n",
      "the won't when that ches be a cang with be mick the it to be the got b\n"
     ]
    }
   ],
   "source": [
    "alphabet = pre.Alphabet(text)\n",
    "\n",
    "HIDDEN_LAYER_SIZE = 128\n",
    "VOCAB_SIZE = alphabet.get_size()\n",
    "TIMESTEPS = 20\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "EMBEDDING_SIZE = 10\n",
    "\n",
    "\n",
    "data, labels = alphabet.making_one_hot(text, TIMESTEPS)\n",
    "# data, labels = alphabet.making_embedded_one_hot(text, TIMESTEPS)\n",
    "\n",
    "# embedding = nn.LeanableEmbedding(name = \"learnable-embedding\")\n",
    "# embedding.build(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "\n",
    "rnn = Single_Layer_LSTM_Classifier(name = \"basic\")\n",
    "rnn.build(HIDDEN_LAYER_SIZE, VOCAB_SIZE, TIMESTEPS, l2_reg=0.0)\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(\"1-Hot-Encoding\", alphabet.word2index)\n",
    "decoder = OneHotDecoder(\"1-Hot-Decoding\", alphabet.index2word)\n",
    "\n",
    "sampler = lambda trainable, seed_text: sample( seed_text, trainable, encoder, decoder, length=100)\n",
    "\n",
    "train( rnn, data, labels, sampler, EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tools.processing as pre\n",
    "import tools.architectures as nn\n",
    "\n",
    "vocabulary_size = 35\n",
    "embedding_dimension = 10\n",
    "\n",
    "_inputs = tf.placeholder(tf.int32, shape=[None, TIMESTEPS])\n",
    "\n",
    "with tf.name_scope(\"embeddings\"):\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size,\n",
    "                           embedding_dimension],\n",
    "                          -1.0, 1.0), name='embedding')\n",
    "    print(embeddings.shape)\n",
    "    print(_inputs.shape)\n",
    "    embed = tf.nn.embedding_lookup(embeddings, _inputs)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    embed = sess.run(embed, feed_dict={_inputs: data})\n",
    "    \n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tools.processing as pre\n",
    "text = pre.get_text(\"data/cleaned-rap-lyrics/clean2_pac_.txt\")\n",
    "alphabet = pre.Alphabet(text)\n",
    "data, labels = alphabet.making_embedded_one_hot(text, 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
