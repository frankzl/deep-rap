{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Introducing the tools\n",
    "\n",
    "We want to outsource some of the functions from the previous notebook to external modules. The train methods for example can be reused, as it is the same for every model\n",
    "\n",
    "At the moment it looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def train(self, train_data, train_labels, alphabet, epochs=20, batch_size=128):\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        \n",
    "        self.session = tf.Session()\n",
    "        session = self.session\n",
    "        \n",
    "        with session.as_default():\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            tr_loss, tr_acc = session.run([self.loss, self.accuracy],\n",
    "                                          feed_dict={self.X: train_data,\n",
    "                                                     self.Y: train_labels})\n",
    "            train_losses.append(tr_loss)\n",
    "            train_accs.append(tr_acc)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                \n",
    "                if(epoch + 1) % 1 == 0:\n",
    "                    print(f\"\\n\\nEpoch {epoch + 1}/{epochs}\")\n",
    "                    print(f\"Loss:    \\t {tr_loss}\")\n",
    "                    print(f\"Accuracy:\\t {tr_acc}\")\n",
    "                \n",
    "                for batch_ixs in batch_data(len(train_data), batch_size):\n",
    "                    _ = session.run(self.train_step,\n",
    "                                   feed_dict={\n",
    "                                       self.X: train_data[batch_ixs],\n",
    "                                       self.Y: train_labels[batch_ixs],\n",
    "                                   })\n",
    "                tr_loss, tr_acc = session.run([self.loss, self.accuracy],\n",
    "                                               feed_dict={self.X: train_data,\n",
    "                                                          self.Y: train_labels\n",
    "                                                         })\n",
    "                train_losses.append(tr_loss)\n",
    "                train_accs.append(tr_acc)\n",
    "                \n",
    "                #get on of training set as seed\n",
    "                seed = train_data[:1:]\n",
    "        \n",
    "                #to print the seed 40 characters\n",
    "                seed_chars = ''\n",
    "                for each in seed[0]:\n",
    "                    seed_chars += alphabet._keys[np.where(each == max(each))[0][0]]\n",
    "                print (\"Seed:\" + seed_chars)\n",
    "        \n",
    "                #predict next 500 characters\n",
    "                for i in range(500):\n",
    "                    if i > 0:\n",
    "                        remove_fist_char = seed[:,1:,:]\n",
    "                        seed = np.append(remove_fist_char, np.reshape(probabilities, [1, 1, self.vocab_size]), axis=1)\n",
    "                        \n",
    "                    predicted = session.run([self.final_output], feed_dict = {self.X:seed})\n",
    "                    predicted = np.asarray(predicted[0]).astype('float64')[0]\n",
    "                    probabilities = sample(predicted)\n",
    "                    predicted_chars = alphabet._keys[np.argmax(probabilities)]\n",
    "                    seed_chars += predicted_chars\n",
    "                print ('Result:'+ seed_chars)\n",
    "        \n",
    "        self.hist = {\n",
    "            'train_losses': np.array(train_losses),\n",
    "            'train_accuracy': np.array(train_accs)\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainable Object\n",
    "The Trainable contains all the attributes needed for training. All our implemeted models from now on should contain these attributes\n",
    "\n",
    "We can now generalize our train function as seen in our module ***tools.architectures***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Trainable:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        \n",
    "        self.final_output = None\n",
    "        self.loss = None\n",
    "        self.accuracy = None\n",
    "        self.train_step = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling function\n",
    "We need a proper sampling function that:\n",
    "1. feeds a seed into the model\n",
    "2. translate output of model into readable text\n",
    "\n",
    "At the moment, we have\n",
    "\n",
    "```python\n",
    "#get on of training set as seed\n",
    "seed = train_data[:1:]\n",
    "\n",
    "#to print the seed 40 characters\n",
    "seed_chars = ''\n",
    "for each in seed[0]:\n",
    "    seed_chars += alphabet._keys[np.where(each == max(each))[0][0]]\n",
    "    print (\"Seed:\" + seed_chars)\n",
    "    \n",
    "#predict next 500 characters\n",
    "for i in range(500):\n",
    "    if i > 0:\n",
    "        remove_fist_char = seed[:,1:,:]\n",
    "        seed = np.append(remove_fist_char, np.reshape(probabilities, [1, 1, self.vocab_size]), axis=1)\n",
    "        \n",
    "    predicted = session.run([self.final_output], feed_dict = {self.X:seed})\n",
    "    predicted = np.asarray(predicted[0]).astype('float64')[0]\n",
    "    probabilities = sample(predicted)\n",
    "    predicted_chars = alphabet._keys[np.argmax(probabilities)]\n",
    "    seed_chars += predicted_chars\n",
    "print ('Result:'+ seed_chars)\n",
    "```\n",
    "\n",
    "What we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample( seed_text, trainable, encoder, decoder, length=40 ):\n",
    "    \n",
    "    \"\"\" prints the sampled string\n",
    "    \n",
    "    seed_text: string of the seed, must have minimum length of our timestep size\n",
    "    \n",
    "    trainable: object model to sample from\n",
    "    \n",
    "    encoder: encoder object to encode the seed_text\n",
    "    \n",
    "    decoder: decoder object to decode the output from the trainable\n",
    "    \n",
    "    length: how many symbols we want to sample\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    seed = encoder.encode( [seed_text] )\n",
    "\n",
    "    #to print the seed characters\n",
    "    seed_chars = seed_text\n",
    "    print( \"------Sampling----------\" )\n",
    "    print( f\"seed: \\t{seed_text}\" )\n",
    "        \n",
    "    #predict next symbols\n",
    "    for i in range(length):\n",
    "        if i > 0:\n",
    "            seed = encoder.encode( [seed_chars] )\n",
    "            # Take only the last required symbols\n",
    "            seed = seed[:,-1*trainable.time_steps:,:]\n",
    "            \n",
    "            # remove_fist_char = seed[:,1:,:]\n",
    "            # seed = np.append(remove_fist_char, np.reshape(probabilities, [1, 1, trainable.vocab_size]), axis=1)\n",
    "            \n",
    "        predicted = trainable.session.run([trainable.final_output], feed_dict = {trainable.X:seed})\n",
    "        predicted = np.asarray(predicted[0]).astype('float64')[0]\n",
    "        \n",
    "        predicted_symbol = decoder.decode( predicted )\n",
    "        seed_chars += predicted_symbol\n",
    "    print ('result:'+ seed_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def encode(self, seed_chars):\n",
    "        pass\n",
    "    \n",
    "class Decoder:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def decode(self, predicted):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now whenever we want to sample during our training process, we pass a sampling function as an argument to our train method.\n",
    "\n",
    "A possible function call could look like:\n",
    "```python\n",
    "encoder = Encoder(\"encoder\")\n",
    "decoder = Decoder(\"decoder\")\n",
    "sampler = lambda trainable, seed_text: sample( seed_text, trainable, encoder, decoder, length=40)\n",
    "\n",
    "train( model, train_data, train_labels, sampler, epochs, batch_size, temperature )\n",
    "\n",
    "```\n",
    "\n",
    "Let's put these functions together inside the train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainable, train_data, train_labels, sampler, epochs=20, batch_size=128):\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    \n",
    "    trainable.session = tf.Session()\n",
    "    session = trainable.session\n",
    "    \n",
    "    with session.as_default():\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        tr_loss, tr_acc = session.run([trainable.loss, trainable.accuracy],\n",
    "                                      feed_dict={trainable.X: train_data,\n",
    "                                                 trainable.Y: train_labels})\n",
    "        train_losses.append(tr_loss)\n",
    "        train_accs.append(tr_acc)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "             \n",
    "            for batch_ixs in batch_data(len(train_data), batch_size):\n",
    "                _ = session.run(trainable.train_step,\n",
    "                               feed_dict={\n",
    "                                   trainable.X: train_data[batch_ixs],\n",
    "                                   trainable.Y: train_labels[batch_ixs],\n",
    "                               })\n",
    "            tr_loss, tr_acc = session.run([trainable.loss, trainable.accuracy],\n",
    "                                           feed_dict={trainable.X: train_data,\n",
    "                                                      trainable.Y: train_labels\n",
    "                                                     })\n",
    "            train_losses.append(tr_loss)\n",
    "            train_accs.append(tr_acc)\n",
    "            \n",
    "            if(epoch + 1) % 1 == 0:\n",
    "                print(f\"\\n\\nEpoch {epoch + 1}/{epochs}\")\n",
    "                print(f\"Loss:    \\t {tr_loss}\")\n",
    "                print(f\"Accuracy:\\t {tr_acc}\")\n",
    "            \n",
    "            \n",
    "            #get on of training set as seed\n",
    "            seed_text = \"as real as it seems the american dream\\nain't nothing but another calculated schemes\\nto get us locked up\"\n",
    "            \n",
    "            sampler(trainable, seed_text[:trainable.time_steps])\n",
    "    \n",
    "    trainable.hist = {\n",
    "        'train_losses': np.array(train_losses),\n",
    "        'train_accuracy': np.array(train_accs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Single_Layer_LSTM_Classifier(Trainable):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "        \n",
    "    def build(self, hidden_layer_size, vocab_size, time_steps, l2_reg=0.0):\n",
    "        self.time_steps = time_steps\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape=[None, time_steps, vocab_size], name=\"data\")\n",
    "        self.Y = tf.placeholder(tf.int16, shape=[None, vocab_size], name=\"labels\")\n",
    "        \n",
    "        _X = tf.transpose(self.X, [1, 0, 2])\n",
    "        _X = tf.reshape(_X, [-1, vocab_size])\n",
    "        _X = tf.split(_X, time_steps, 0)\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            # 1x RNN LSTM Cell\n",
    "            self.rnn_cell   = tf.nn.rnn_cell.LSTMCell(hidden_layer_size)\n",
    "            \n",
    "            self.outputs, _ = tf.contrib.rnn.static_rnn(self.rnn_cell, _X, dtype=tf.float32)\n",
    "            \n",
    "            # 1x linear output layer\n",
    "            W_out = tf.Variable(tf.truncated_normal([hidden_layer_size, vocab_size], \n",
    "                                                 mean=0, stddev=.01))\n",
    "            b_out = tf.Variable(tf.truncated_normal([vocab_size],\n",
    "                                                mean=0, stddev=.01))\n",
    "            self.weights.append(W_out)\n",
    "            self.biases.append(b_out)\n",
    "            \n",
    "            self.last_rnn_output = self.outputs[-1]\n",
    "            self.final_output    = self.last_rnn_output @ W_out + b_out\n",
    "            \n",
    "            # softmax cross entropy as our loss function (between 36 classes)\n",
    "            self.softmax = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.final_output,\n",
    "                                                                labels=self.Y)\n",
    "            self.cross_entropy_loss = tf.reduce_mean(self.softmax)\n",
    "            \n",
    "            self.loss = self.cross_entropy_loss\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer()\n",
    "            self.train_step= self.optimizer.minimize(self.loss)\n",
    "            \n",
    "            self.correct_prediction = tf.equal(tf.argmax(self.Y,1), tf.argmax(self.final_output, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))*100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(num_data, batch_size):\n",
    "    \"\"\" Yield batches with indices until epoch is over.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_data: int\n",
    "        The number of samples in the dataset.\n",
    "    batch_size: int\n",
    "        The batch size used using training.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    batch_ixs: np.array of ints with shape [batch_size,]\n",
    "        Yields arrays of indices of size of the batch size until the epoch is over.\n",
    "    \"\"\"\n",
    "    \n",
    "    # data_ixs = np.random.permutation(np.arange(num_data))\n",
    "    data_ixs = np.arange(num_data)\n",
    "    ix = 0\n",
    "    while ix + batch_size < num_data:\n",
    "        batch_ixs = data_ixs[ix:ix+batch_size]\n",
    "        ix += batch_size\n",
    "        yield batch_ixs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools.processing as pre\n",
    "import tools.architectures as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_distribution(predicted, temperature=0.9):\n",
    "    '''\n",
    "     helper function to sample an index from a probability array\n",
    "     our model will output scores for each class\n",
    "     we normalize those outputs and create a probability distribution out of them to sample from\n",
    "    '''\n",
    "    exp_predicted = np.exp(predicted/temperature)\n",
    "    predicted = exp_predicted / np.sum(exp_predicted)\n",
    "    probabilities = np.random.multinomial(1, predicted, 1)\n",
    "    return probabilities\n",
    "\n",
    "class OneHotEncoder(Encoder):\n",
    "    \"\"\"\n",
    "    Encodes sequences of words to sequences of 1-Hot Encoded vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name, word2index):\n",
    "        super(OneHotEncoder, self).__init__(name)\n",
    "        self.word2index = word2index\n",
    "        \n",
    "    def encode(self, sequences):\n",
    "        encoded_sequences = []\n",
    "        for seq in sequences:\n",
    "            encoded = np.zeros( ( len(seq), len(self.word2index) ) )\n",
    "            \n",
    "            for idx, symbol in enumerate(seq):\n",
    "                encoded[idx][ self.word2index[symbol] ] = 1\n",
    "            \n",
    "            encoded_sequences.append(encoded)\n",
    "        \n",
    "        return np.array(encoded_sequences)\n",
    "    \n",
    "    def encode_labels(self, labels):\n",
    "        \n",
    "        encoded = []\n",
    "        \n",
    "        for label in labels:\n",
    "            one_hot_vec = np.zeros(len(self.word2index), dtype=int)\n",
    "            one_hot_vec[ self.word2index[label] ] = 1\n",
    "            encoded.append( one_hot_vec )\n",
    "            \n",
    "        return np.array(encoded)\n",
    "    \n",
    "class OneHotDecoder(Decoder):\n",
    "    \"\"\"\n",
    "    Decodes a 1-Hot Encoded vector (prediction) to a word\n",
    "    \"\"\"\n",
    "    def __init__(self, name, index2word, temperature=0.5):\n",
    "        super(OneHotDecoder, self).__init__(name)\n",
    "        self.temperature = temperature\n",
    "        self.index2word = index2word \n",
    "        \n",
    "    def decode(self, predicted):\n",
    "        predicted = sample_from_distribution(predicted, temperature=self.temperature)\n",
    "        return self.index2word[ np.argmax(predicted) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('as real as it seems ', 't'), ('s real as it seems t', 'h'), (' real as it seems th', 'e'), ('real as it seems the', ' '), ('eal as it seems the ', 'a')]\n"
     ]
    }
   ],
   "source": [
    "def create_data_label_pairs(text, time_steps, step=1):\n",
    "    '''\n",
    "    creates data-label pairs from the given text\n",
    "    '''\n",
    "    input_chars = []\n",
    "    output_char = []\n",
    "    \n",
    "    for i in range(0, len(text) - time_steps, step):\n",
    "        input_chars.append(text[i:i+time_steps])\n",
    "        output_char.append(text[i+time_steps])\n",
    "    return input_chars, output_char\n",
    "\n",
    "text = pre.get_text(\"data/cleaned-rap-lyrics/clean2_pac_.txt\")\n",
    "alphabet = pre.Alphabet(text)\n",
    "\n",
    "TIMESTEPS = 20\n",
    "\n",
    "str_data, str_labels = create_data_label_pairs(text, TIMESTEPS)\n",
    "\n",
    "print( list( zip(str_data, str_labels) )[:5] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now use our encoder to encode these pairs of data and label strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(\"1-Hot-Encoding\", alphabet.word2index)\n",
    "decoder = OneHotDecoder(\"1-Hot-Decoding\", alphabet.index2word)\n",
    "\n",
    "data = encoder.encode( str_data )\n",
    "labels = encoder.encode_labels( str_labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1/20\n",
      "Loss:    \t 2.899674415588379\n",
      "Accuracy:\t 17.976741790771484\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems   n  r ee ttt he i f afet u m   hetnle ate e\n",
      "i  t oo n ea at et lseh o h  ane t t t te eauta t ets t\n",
      "\n",
      "\n",
      "Epoch 2/20\n",
      "Loss:    \t 2.6095852851867676\n",
      "Accuracy:\t 26.142301559448242\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems thor yiule nhew inh tin le i the then ges i re dat toag'e ni tor  heti thet yhe the the  eu thi toe \n",
      "\n",
      "\n",
      "Epoch 3/20\n",
      "Loss:    \t 2.4637558460235596\n",
      "Accuracy:\t 28.608226776123047\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems the foa ghe dhe dan then than the ther int in the thous the the set's met the fiche soe iot at the g\n",
      "\n",
      "\n",
      "Epoch 4/20\n",
      "Loss:    \t 2.3885746002197266\n",
      "Accuracy:\t 30.31136703491211\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems he bange than be bafd top the be sot on an an the bu pir the then oa doule the thon the bank an mas \n",
      "\n",
      "\n",
      "Epoch 5/20\n",
      "Loss:    \t 2.329887866973877\n",
      "Accuracy:\t 31.271726608276367\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems yhan the bedsen wa w in thet in gom ther ine the souns the ceop the the sot bos the the bow re goun \n",
      "\n",
      "\n",
      "Epoch 6/20\n",
      "Loss:    \t 2.2817256450653076\n",
      "Accuracy:\t 31.779417037963867\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems me the wars me bathe the the whe de sout yor in whe wouns ame fore sisle don' the care gout the sean\n",
      "\n",
      "\n",
      "Epoch 7/20\n",
      "Loss:    \t 2.2402493953704834\n",
      "Accuracy:\t 32.55470657348633\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems in yous me me fow bos che the therte at to ghand i' in that an then the git pare lot a me\n",
      "the weas o\n",
      "\n",
      "\n",
      "Epoch 8/20\n",
      "Loss:    \t 2.203859806060791\n",
      "Accuracy:\t 33.31499481201172\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems the bunge sound the can ink nnon the ind they when my ut the buthe beat me whan i'm whet soun the th\n",
      "\n",
      "\n",
      "Epoch 9/20\n",
      "Loss:    \t 2.171614646911621\n",
      "Accuracy:\t 34.57546615600586\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems an whersing the sout stou hock rrathe ther ally the lith no buthey i'm me buspe the thet\n",
      "go gom ange\n",
      "\n",
      "\n",
      "Epoch 10/20\n",
      "Loss:    \t 2.1465506553649902\n",
      "Accuracy:\t 35.52082061767578\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems a mese fan't al a ge storpy the mang am and bee the bett fa the mighan sto gaf i the pean the me but\n",
      "\n",
      "\n",
      "Epoch 11/20\n",
      "Loss:    \t 2.118622303009033\n",
      "Accuracy:\t 36.42866134643555\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems the sous the wami he buthe the whan and thin' thit shen mase the buan shen you ssack of ther cald th\n",
      "\n",
      "\n",
      "Epoch 12/20\n",
      "Loss:    \t 2.0943377017974854\n",
      "Accuracy:\t 37.45154571533203\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems dous the wathey seet i wan it the the shon the the goge\n",
      "i'm the the donget me the the a bane\n",
      "thit la\n",
      "\n",
      "\n",
      "Epoch 13/20\n",
      "Loss:    \t 2.0713412761688232\n",
      "Accuracy:\t 38.11178970336914\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems petthe the calle a me my got a gon the bup the the bus the hing tou mus star i get couse that i thin\n",
      "\n",
      "\n",
      "Epoch 14/20\n",
      "Loss:    \t 2.049729108810425\n",
      "Accuracy:\t 38.789546966552734\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems and caus the that the get i caus the found the touse the the they that you fous they gat the got me \n",
      "\n",
      "\n",
      "Epoch 15/20\n",
      "Loss:    \t 2.029153347015381\n",
      "Accuracy:\t 39.399776458740234\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems i gett nould 'me burt the pas on ke wher he mede the purken' and the the the furnin' for the got the\n",
      "\n",
      "\n",
      "Epoch 16/20\n",
      "Loss:    \t 2.009363889694214\n",
      "Accuracy:\t 39.9849967956543\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems and bas hing and and bo bel an i me the i got the sheas the gon the suy ming the but me but the got \n",
      "\n",
      "\n",
      "Epoch 17/20\n",
      "Loss:    \t 1.9916986227035522\n",
      "Accuracy:\t 40.22758483886719\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems the bust to buck your ghang the me the weat i get cous the lith now she but hor i me an at west a lo\n",
      "\n",
      "\n",
      "Epoch 18/20\n",
      "Loss:    \t 1.9729429483413696\n",
      "Accuracy:\t 40.58771896362305\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems cale a cauth\n",
      "reit the wack chan he but\n",
      "i gat mos the but they the the wigg to beat the the they a ca\n",
      "\n",
      "\n",
      "Epoch 19/20\n",
      "Loss:    \t 1.9560914039611816\n",
      "Accuracy:\t 40.88783264160156\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems i got to just on whem it me the but ne blathe bust i stous i bettir i got they hin't mo reals\n",
      "when b\n",
      "\n",
      "\n",
      "Epoch 20/20\n",
      "Loss:    \t 1.9395740032196045\n",
      "Accuracy:\t 41.385520935058594\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems the my a bus the get when the wing a the a seall the liggin' lean and the peat to get i was in the b\n"
     ]
    }
   ],
   "source": [
    "\n",
    "HIDDEN_LAYER_SIZE = 128\n",
    "VOCAB_SIZE = alphabet.get_size()\n",
    "TIMESTEPS = 20\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "EMBEDDING_SIZE = 10\n",
    "\n",
    "# data, labels = alphabet.making_one_hot(text, TIMESTEPS)\n",
    "\n",
    "rnn = Single_Layer_LSTM_Classifier(name = \"basic\")\n",
    "rnn.build(HIDDEN_LAYER_SIZE, VOCAB_SIZE, TIMESTEPS, l2_reg=0.0)\n",
    "\n",
    "sampler = lambda trainable, seed_text: sample( seed_text, trainable, encoder, decoder, length=100)\n",
    "\n",
    "train( rnn, data, labels, sampler, EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learnings\n",
    "We now export all of these functions into a separate module called tools.architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
