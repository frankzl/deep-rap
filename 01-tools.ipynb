{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Introducing the tools\n",
    "\n",
    "We want to outsource some of the functions from the previous notebook to external modules. The train methods for example can be reused, as it is the same for every model\n",
    "\n",
    "At the moment it looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def train(self, train_data, train_labels, alphabet, epochs=20, batch_size=128):\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        \n",
    "        self.session = tf.Session()\n",
    "        session = self.session\n",
    "        \n",
    "        with session.as_default():\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            tr_loss, tr_acc = session.run([self.loss, self.accuracy],\n",
    "                                          feed_dict={self.X: train_data,\n",
    "                                                     self.Y: train_labels})\n",
    "            train_losses.append(tr_loss)\n",
    "            train_accs.append(tr_acc)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                \n",
    "                if(epoch + 1) % 1 == 0:\n",
    "                    print(f\"\\n\\nEpoch {epoch + 1}/{epochs}\")\n",
    "                    print(f\"Loss:    \\t {tr_loss}\")\n",
    "                    print(f\"Accuracy:\\t {tr_acc}\")\n",
    "                \n",
    "                for batch_ixs in batch_data(len(train_data), batch_size):\n",
    "                    _ = session.run(self.train_step,\n",
    "                                   feed_dict={\n",
    "                                       self.X: train_data[batch_ixs],\n",
    "                                       self.Y: train_labels[batch_ixs],\n",
    "                                   })\n",
    "                tr_loss, tr_acc = session.run([self.loss, self.accuracy],\n",
    "                                               feed_dict={self.X: train_data,\n",
    "                                                          self.Y: train_labels\n",
    "                                                         })\n",
    "                train_losses.append(tr_loss)\n",
    "                train_accs.append(tr_acc)\n",
    "                \n",
    "                #get on of training set as seed\n",
    "                seed = train_data[:1:]\n",
    "        \n",
    "                #to print the seed 40 characters\n",
    "                seed_chars = ''\n",
    "                for each in seed[0]:\n",
    "                    seed_chars += alphabet._keys[np.where(each == max(each))[0][0]]\n",
    "                print (\"Seed:\" + seed_chars)\n",
    "        \n",
    "                #predict next 500 characters\n",
    "                for i in range(500):\n",
    "                    if i > 0:\n",
    "                        remove_fist_char = seed[:,1:,:]\n",
    "                        seed = np.append(remove_fist_char, np.reshape(probabilities, [1, 1, self.vocab_size]), axis=1)\n",
    "                        \n",
    "                    predicted = session.run([self.final_output], feed_dict = {self.X:seed})\n",
    "                    predicted = np.asarray(predicted[0]).astype('float64')[0]\n",
    "                    probabilities = sample(predicted)\n",
    "                    predicted_chars = alphabet._keys[np.argmax(probabilities)]\n",
    "                    seed_chars += predicted_chars\n",
    "                print ('Result:'+ seed_chars)\n",
    "        \n",
    "        self.hist = {\n",
    "            'train_losses': np.array(train_losses),\n",
    "            'train_accuracy': np.array(train_accs)\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainable Object\n",
    "The Trainable contains all the attributes needed for training. All our implemeted models from now on should contain these attributes\n",
    "\n",
    "We can now generalize our train function as seen in our module ***tools.architectures***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Trainable:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        \n",
    "        self.final_output = None\n",
    "        self.loss = None\n",
    "        self.accuracy = None\n",
    "        self.train_step = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling function\n",
    "We need a proper sampling function that:\n",
    "1. feeds a seed into the model\n",
    "2. translate output of model into readable text\n",
    "\n",
    "At the moment, we have\n",
    "\n",
    "```python\n",
    "#get on of training set as seed\n",
    "seed = train_data[:1:]\n",
    "\n",
    "#to print the seed 40 characters\n",
    "seed_chars = ''\n",
    "for each in seed[0]:\n",
    "    seed_chars += alphabet._keys[np.where(each == max(each))[0][0]]\n",
    "    print (\"Seed:\" + seed_chars)\n",
    "    \n",
    "#predict next 500 characters\n",
    "for i in range(500):\n",
    "    if i > 0:\n",
    "        remove_fist_char = seed[:,1:,:]\n",
    "        seed = np.append(remove_fist_char, np.reshape(probabilities, [1, 1, self.vocab_size]), axis=1)\n",
    "        \n",
    "    predicted = session.run([self.final_output], feed_dict = {self.X:seed})\n",
    "    predicted = np.asarray(predicted[0]).astype('float64')[0]\n",
    "    probabilities = sample(predicted)\n",
    "    predicted_chars = alphabet._keys[np.argmax(probabilities)]\n",
    "    seed_chars += predicted_chars\n",
    "print ('Result:'+ seed_chars)\n",
    "```\n",
    "\n",
    "What we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample( seed_text, trainable, encoder, decoder, length=40 ):\n",
    "    \n",
    "    \"\"\" prints the sampled string\n",
    "    \n",
    "    seed_text: string of the seed, must have minimum length of our timestep size\n",
    "    \n",
    "    trainable: object model to sample from\n",
    "    \n",
    "    encoder: encoder object to encode the seed_text\n",
    "    \n",
    "    decoder: decoder object to decode the output from the trainable\n",
    "    \n",
    "    length: how many symbols we want to sample\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    seed = encoder.encode_raw( seed_text )\n",
    "\n",
    "    #to print the seed characters\n",
    "    seed_chars = seed_text\n",
    "    print( \"------Sampling----------\" )\n",
    "    print( f\"seed: \\t{seed_text}\" )\n",
    "        \n",
    "    #predict next symbols\n",
    "    for i in range(length):\n",
    "        if i > 0:\n",
    "            seed = encoder.encode_raw( seed_chars )\n",
    "            # Take only the last required symbols\n",
    "            seed = seed[:,-1*trainable.time_steps:,:]\n",
    "            \n",
    "            # remove_fist_char = seed[:,1:,:]\n",
    "            # seed = np.append(remove_fist_char, np.reshape(probabilities, [1, 1, trainable.vocab_size]), axis=1)\n",
    "            \n",
    "        predicted = trainable.session.run([trainable.final_output], feed_dict = {trainable.X:seed})\n",
    "        predicted = np.asarray(predicted[0]).astype('float64')[0]\n",
    "        \n",
    "        predicted_symbol = decoder.decode( predicted )\n",
    "        seed_chars += predicted_symbol\n",
    "    print ('result:'+ seed_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def encode(self, seed_chars):\n",
    "        pass\n",
    "    def encode_raw(self, seed_text):\n",
    "        pass\n",
    "    \n",
    "class Decoder:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def decode(self, predicted):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now whenever we want to sample during our training process, we pass a sampling function as an argument to our train method.\n",
    "\n",
    "A possible function call could look like:\n",
    "```python\n",
    "encoder = Encoder(\"encoder\")\n",
    "decoder = Decoder(\"decoder\")\n",
    "sampler = lambda trainable, seed_text: sample( seed_text, trainable, encoder, decoder, length=40)\n",
    "\n",
    "train( model, train_data, train_labels, sampler, epochs, batch_size, temperature )\n",
    "\n",
    "```\n",
    "\n",
    "Let's put these functions together inside the train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainable, train_data, train_labels, sampler, epochs=20, batch_size=128):\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    \n",
    "    trainable.session = tf.Session()\n",
    "    session = trainable.session\n",
    "    \n",
    "    with session.as_default():\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        tr_loss, tr_acc = session.run([trainable.loss, trainable.accuracy],\n",
    "                                      feed_dict={trainable.X: train_data,\n",
    "                                                 trainable.Y: train_labels})\n",
    "        train_losses.append(tr_loss)\n",
    "        train_accs.append(tr_acc)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "             \n",
    "            for batch_ixs in batch_data(len(train_data), batch_size):\n",
    "                _ = session.run(trainable.train_step,\n",
    "                               feed_dict={\n",
    "                                   trainable.X: train_data[batch_ixs],\n",
    "                                   trainable.Y: train_labels[batch_ixs],\n",
    "                               })\n",
    "            tr_loss, tr_acc = session.run([trainable.loss, trainable.accuracy],\n",
    "                                           feed_dict={trainable.X: train_data,\n",
    "                                                      trainable.Y: train_labels\n",
    "                                                     })\n",
    "            train_losses.append(tr_loss)\n",
    "            train_accs.append(tr_acc)\n",
    "            \n",
    "            if(epoch + 1) % 1 == 0:\n",
    "                print(f\"\\n\\nEpoch {epoch + 1}/{epochs}\")\n",
    "                print(f\"Loss:    \\t {tr_loss}\")\n",
    "                print(f\"Accuracy:\\t {tr_acc}\")\n",
    "            \n",
    "            \n",
    "            #get on of training set as seed\n",
    "            seed_text = \"as real as it seems the american dream\\nain't nothing but another calculated schemes\\nto get us locked up\"\n",
    "            \n",
    "            sampler(trainable, seed_text[:trainable.time_steps])\n",
    "    \n",
    "    trainable.hist = {\n",
    "        'train_losses': np.array(train_losses),\n",
    "        'train_accuracy': np.array(train_accs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Single_Layer_LSTM_Classifier(Trainable):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "        \n",
    "    def build(self, hidden_layer_size, vocab_size, time_steps, l2_reg=0.0):\n",
    "        self.time_steps = time_steps\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape=[None, time_steps, vocab_size], name=\"data\")\n",
    "        self.Y = tf.placeholder(tf.int16, shape=[None, vocab_size], name=\"labels\")\n",
    "        \n",
    "        _X = tf.transpose(self.X, [1, 0, 2])\n",
    "        _X = tf.reshape(_X, [-1, vocab_size])\n",
    "        _X = tf.split(_X, time_steps, 0)\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            # 1x RNN LSTM Cell\n",
    "            self.rnn_cell   = tf.nn.rnn_cell.LSTMCell(hidden_layer_size)\n",
    "            \n",
    "            self.outputs, _ = tf.contrib.rnn.static_rnn(self.rnn_cell, _X, dtype=tf.float32)\n",
    "            \n",
    "            # 1x linear output layer\n",
    "            W_out = tf.Variable(tf.truncated_normal([hidden_layer_size, vocab_size], \n",
    "                                                 mean=0, stddev=.01))\n",
    "            b_out = tf.Variable(tf.truncated_normal([vocab_size],\n",
    "                                                mean=0, stddev=.01))\n",
    "            self.weights.append(W_out)\n",
    "            self.biases.append(b_out)\n",
    "            \n",
    "            self.last_rnn_output = self.outputs[-1]\n",
    "            self.final_output    = self.last_rnn_output @ W_out + b_out\n",
    "            \n",
    "            # softmax cross entropy as our loss function (between 36 classes)\n",
    "            self.softmax = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.final_output,\n",
    "                                                                labels=self.Y)\n",
    "            self.cross_entropy_loss = tf.reduce_mean(self.softmax)\n",
    "            \n",
    "            self.loss = self.cross_entropy_loss\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer()\n",
    "            self.train_step= self.optimizer.minimize(self.loss)\n",
    "            \n",
    "            self.correct_prediction = tf.equal(tf.argmax(self.Y,1), tf.argmax(self.final_output, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))*100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(num_data, batch_size):\n",
    "    \"\"\" Yield batches with indices until epoch is over.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_data: int\n",
    "        The number of samples in the dataset.\n",
    "    batch_size: int\n",
    "        The batch size used using training.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    batch_ixs: np.array of ints with shape [batch_size,]\n",
    "        Yields arrays of indices of size of the batch size until the epoch is over.\n",
    "    \"\"\"\n",
    "    \n",
    "    # data_ixs = np.random.permutation(np.arange(num_data))\n",
    "    data_ixs = np.arange(num_data)\n",
    "    ix = 0\n",
    "    while ix + batch_size < num_data:\n",
    "        batch_ixs = data_ixs[ix:ix+batch_size]\n",
    "        ix += batch_size\n",
    "        yield batch_ixs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools.processing as pre\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_distribution(predicted, temperature=0.9):\n",
    "    '''\n",
    "     helper function to sample an index from a probability array\n",
    "     our model will output scores for each class\n",
    "     we normalize those outputs and create a probability distribution out of them to sample from\n",
    "    '''\n",
    "    exp_predicted = np.exp(predicted/temperature)\n",
    "    predicted = exp_predicted / np.sum(exp_predicted)\n",
    "    probabilities = np.random.multinomial(1, predicted, 1)\n",
    "    return probabilities\n",
    "\n",
    "class OneHotEncoder(Encoder):\n",
    "    \"\"\"\n",
    "    Encodes sequences of words to sequences of 1-Hot Encoded vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name, word2index):\n",
    "        super(OneHotEncoder, self).__init__(name)\n",
    "        self.word2index = word2index\n",
    "        \n",
    "    def encode(self, sequences):\n",
    "        encoded_sequences = []\n",
    "        for seq in sequences:\n",
    "            encoded = np.zeros( ( len(seq), len(self.word2index) ) )\n",
    "            \n",
    "            for idx, symbol in enumerate(seq):\n",
    "                encoded[idx][ self.word2index[symbol] ] = 1\n",
    "            \n",
    "            encoded_sequences.append(encoded)\n",
    "        \n",
    "        return np.array(encoded_sequences)\n",
    "    \n",
    "    def encode_raw(self, seed_text):\n",
    "        return self.encode([seed_text])\n",
    "    \n",
    "    def encode_labels(self, labels):\n",
    "        \n",
    "        encoded = []\n",
    "        \n",
    "        for label in labels:\n",
    "            one_hot_vec = np.zeros(len(self.word2index), dtype=int)\n",
    "            one_hot_vec[ self.word2index[label] ] = 1\n",
    "            encoded.append( one_hot_vec )\n",
    "            \n",
    "        return np.array(encoded)\n",
    "    \n",
    "class OneHotDecoder(Decoder):\n",
    "    \"\"\"\n",
    "    Decodes a 1-Hot Encoded vector (prediction) to a word\n",
    "    \"\"\"\n",
    "    def __init__(self, name, index2word, temperature=0.5):\n",
    "        super(OneHotDecoder, self).__init__(name)\n",
    "        self.temperature = temperature\n",
    "        self.index2word = index2word \n",
    "        \n",
    "    def decode(self, predicted):\n",
    "        predicted = sample_from_distribution(predicted, temperature=self.temperature)\n",
    "        return self.index2word[ np.argmax(predicted) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('as real as it seems ', 't'), ('s real as it seems t', 'h'), (' real as it seems th', 'e'), ('real as it seems the', ' '), ('eal as it seems the ', 'a')]\n"
     ]
    }
   ],
   "source": [
    "def create_data_label_pairs(text, time_steps, step=1):\n",
    "    '''\n",
    "    creates data-label pairs from the given text\n",
    "    '''\n",
    "    input_chars = []\n",
    "    output_char = []\n",
    "    \n",
    "    for i in range(0, len(text) - time_steps, step):\n",
    "        input_chars.append(text[i:i+time_steps])\n",
    "        output_char.append(text[i+time_steps])\n",
    "    return input_chars, output_char\n",
    "\n",
    "text = pre.get_text(\"data/cleaned-rap-lyrics/clean2_pac_.txt\")\n",
    "alphabet = pre.Alphabet(text)\n",
    "\n",
    "TIMESTEPS = 20\n",
    "\n",
    "str_data, str_labels = create_data_label_pairs(text, TIMESTEPS)\n",
    "\n",
    "print( list( zip(str_data, str_labels) )[:5] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now use our encoder to encode these pairs of data and label strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(\"1-Hot-Encoding\", alphabet.word2index)\n",
    "decoder = OneHotDecoder(\"1-Hot-Decoding\", alphabet.index2word)\n",
    "\n",
    "data = encoder.encode( str_data )\n",
    "labels = encoder.encode_labels( str_labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60776, 37)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1/20\n",
      "Loss:    \t 2.876185894012451\n",
      "Accuracy:\t 17.976741790771484\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems el t t ootr h tn h ouo eeeeon t thob wo e e t ute 'soilh le o ae  ec ertavt ykssh tou t th d  at t t\n",
      "\n",
      "\n",
      "Epoch 2/20\n",
      "Loss:    \t 2.557581901550293\n",
      "Accuracy:\t 27.6153564453125\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems 'o gel  roug the thei to son in the ge ate eh gos it me the an\n",
      "s uun the toa c nt at to  ye ti th th\n",
      "\n",
      "\n",
      "Epoch 3/20\n",
      "Loss:    \t 2.4249424934387207\n",
      "Accuracy:\t 29.76866340637207\n",
      "------Sampling----------\n",
      "seed: \tas real as it seems \n",
      "result:as real as it seems he  oe the than the her tia rhem hot an ther thes at the tou got al in the the the bas whed wan' the\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-13666cffb228>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_text\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mseed_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-8a0ab3210394>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(trainable, train_data, train_labels, sampler, epochs, batch_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m                                feed_dict={\n\u001b[1;32m     21\u001b[0m                                    \u001b[0mtrainable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_ixs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                                    \u001b[0mtrainable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_ixs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                                })\n\u001b[1;32m     24\u001b[0m             tr_loss, tr_acc = session.run([trainable.loss, trainable.accuracy],\n",
      "\u001b[0;32m~/.envs/env36-ml/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.envs/env36-ml/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.envs/env36-ml/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.envs/env36-ml/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.envs/env36-ml/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.envs/env36-ml/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "HIDDEN_LAYER_SIZE = 128\n",
    "VOCAB_SIZE = alphabet.get_size()\n",
    "TIMESTEPS = 20\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# data, labels = alphabet.making_one_hot(text, TIMESTEPS)\n",
    "\n",
    "rnn = Single_Layer_LSTM_Classifier(name = \"basic\")\n",
    "rnn.build(HIDDEN_LAYER_SIZE, VOCAB_SIZE, TIMESTEPS, l2_reg=0.0)\n",
    "\n",
    "sampler = lambda trainable, seed_text: sample( seed_text, trainable, encoder, decoder, length=100)\n",
    "\n",
    "train( rnn, data, labels, sampler, EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learnings\n",
    "We now export all of these functions into a separate module called **tools.architecture** and **tools.training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
