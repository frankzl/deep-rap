{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tools.processing as pre\n",
    "import tools.embedding as emb\n",
    "import tools.architectures as nn\n",
    "import tools.training as tr\n",
    "import tools.phonetics as phon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embedding for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03209195  0.06766261  0.04151145 ... -0.046448   -0.03842104\n",
      "  -0.04822   ]\n",
      " [ 0.01169672  0.02784324 -0.02134047 ... -0.00254565  0.01072712\n",
      "   0.00599443]\n",
      " [ 0.00825335  0.00466938 -0.03945393 ... -0.0567189  -0.02748226\n",
      "   0.0531178 ]\n",
      " ...\n",
      " [-0.04762312  0.04288607 -0.09683317 ...  0.01158441  0.01227157\n",
      "   0.01355943]\n",
      " [-0.02130833  0.059227   -0.10000064 ... -0.01137079  0.09158197\n",
      "   0.0108908 ]\n",
      " [-0.0196528   0.05993839 -0.09010126 ...  0.00900052  0.04102224\n",
      "  -0.00091614]]\n"
     ]
    }
   ],
   "source": [
    "text = pre.get_text(\"data/cleaned-rap-lyrics/final_2_pac_rakim_kid_cudi.txt\")[:250]\n",
    "\n",
    "vocab = pre.Vocabulary(text)\n",
    "word2index = vocab.word2index\n",
    "index2word = vocab.index2word\n",
    "VOCAB_SIZE = len(index2word)\n",
    "\n",
    "# create embedding for words\n",
    "word_embedding_matrix = emb.get_embedding_matrix(word2index, VOCAB_SIZE)\n",
    "print(word_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embedding for phonems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.83106965 -0.53812933  0.14049865]\n",
      " [-0.57452571 -0.45615974  0.67958683]\n",
      " [-0.85238743 -0.27057332 -0.4474659 ]\n",
      " [-0.89024484 -0.43297064  0.14142321]\n",
      " [-0.889552   -0.42086163  0.17768735]\n",
      " [-0.84767091  0.20958452 -0.48736873]\n",
      " [-0.80071777 -0.59023607 -0.10233522]\n",
      " [-0.8041234  -0.43067229  0.40976447]\n",
      " [-0.85063279 -0.25427106 -0.46018487]\n",
      " [-0.83106846 -0.49368501 -0.25612572]\n",
      " [-0.77628517 -0.57696062 -0.25396407]\n",
      " [-0.85525501  0.34369221  0.38783291]\n",
      " [-0.72286534 -0.66972053  0.17011791]\n",
      " [-0.90681714  0.07853168 -0.41414431]\n",
      " [-0.84486431 -0.38346329 -0.37304166]\n",
      " [-0.73923445 -0.57153291 -0.35620561]\n",
      " [-0.94168699  0.08752931  0.32490653]\n",
      " [ 0.91624618 -0.1400506  -0.37533814]\n",
      " [-0.88760549 -0.45781818 -0.05058585]\n",
      " [-0.95192575  0.20271704 -0.22965866]\n",
      " [-0.88998002  0.08347418 -0.44829413]\n",
      " [-0.1288805  -0.28575739 -0.94959605]\n",
      " [-0.87844598 -0.46678743 -0.10218589]\n",
      " [-0.99973291  0.0226963   0.00435353]\n",
      " [-0.92469871  0.07278427  0.37367749]\n",
      " [-0.92872733  0.10954829  0.35420984]\n",
      " [-0.98819238  0.08642044  0.12651959]\n",
      " [-0.88798094 -0.05522604  0.45655221]\n",
      " [-0.71483773 -0.66277325  0.22302185]\n",
      " [-0.77870333  0.09508178  0.62014556]\n",
      " [-0.82572544 -0.36834463 -0.42719984]\n",
      " [-0.96458721 -0.03701936  0.26115352]\n",
      " [-0.75869954 -0.52236611  0.3892411 ]\n",
      " [-0.89848369 -0.42166027 -0.12218721]\n",
      " [-0.85189265 -0.49740666 -0.16390714]\n",
      " [-0.08865134  0.95458961  0.28442836]\n",
      " [-0.92101055 -0.30935183 -0.23673016]\n",
      " [-0.9315502  -0.20965256  0.29708615]\n",
      " [-0.82505924 -0.51443595 -0.23373653]\n",
      " [-0.83033556 -0.46001527  0.3145296 ]\n",
      " [-0.94038957  0.13505612  0.31213349]\n",
      " [-0.91232622  0.25985038  0.31644672]\n",
      " [-0.89272779  0.00176275  0.45059285]\n",
      " [-0.87289619 -0.1406185  -0.46720299]\n",
      " [-0.65745056 -0.73210901  0.17825575]\n",
      " [-0.80017054 -0.57691598 -0.16399632]\n",
      " [-0.92555779 -0.04088644 -0.37639201]\n",
      " [-0.91765714  0.09767944 -0.38518035]\n",
      " [-0.955446   -0.16260955 -0.2463353 ]\n",
      " [-0.84971929 -0.46367732 -0.25095904]\n",
      " [-0.83528256  0.25620309  0.48648021]\n",
      " [-0.90798557  0.01761704  0.41863087]\n",
      " [-0.92834795  0.13178164  0.34756806]\n",
      " [-0.92108727  0.05003754  0.38612777]\n",
      " [-0.85894579 -0.10602354  0.5009703 ]\n",
      " [-0.94464797  0.2781046   0.17406319]\n",
      " [-0.90446991  0.4144173  -0.10095845]\n",
      " [-0.83686346 -0.54681742 -0.02549712]\n",
      " [-0.96190029  0.15010665 -0.22850786]\n",
      " [-0.89484626 -0.25758079 -0.36455774]\n",
      " [-0.88388461 -0.35842159 -0.30046943]\n",
      " [-0.79716337 -0.2344465   0.55638605]\n",
      " [ 0.81084776 -0.57908005  0.08480806]\n",
      " [-0.9342941   0.33684736 -0.116741  ]\n",
      " [-0.77812654 -0.12117996  0.61630702]\n",
      " [-0.9283123   0.06958895  0.36523083]\n",
      " [-0.95217842  0.22846287  0.20288117]\n",
      " [-0.91551638 -0.12399001  0.38269579]\n",
      " [-0.89139616  0.03926151  0.45152137]\n",
      " [-0.96463948  0.15677732  0.21187623]\n",
      " [-0.93844485  0.07164304  0.3379181 ]\n",
      " [ 0.10141908 -0.98955452 -0.10245005]\n",
      " [ 0.7182911  -0.63876063 -0.2757585 ]\n",
      " [-0.91440481 -0.31957087 -0.24847195]\n",
      " [-0.81720048 -0.47572622  0.32537353]\n",
      " [-0.47011715 -0.12233038 -0.87408537]\n",
      " [-0.88914901 -0.15891369 -0.42913917]\n",
      " [-0.86603016  0.03746723 -0.49858603]\n",
      " [-0.68690002  0.23986557  0.68602699]\n",
      " [-0.89934129  0.12577032  0.4187685 ]\n",
      " [-0.8662762  -0.01368999  0.49937788]\n",
      " [-0.92921001  0.3657164  -0.05310719]\n",
      " [-0.95402688  0.04104564  0.29689705]\n",
      " [-0.53371918 -0.81742424 -0.2167059 ]\n",
      " [-0.77139229 -0.16222043  0.03486567]]\n"
     ]
    }
   ],
   "source": [
    "phonems = pre.get_text(\"data/phonem-rap-lyrics/phonem_all.txt\")\n",
    "\n",
    "vocab_phonem = pre.Vocabulary(phonems)\n",
    "phonem2index = vocab_phonem.word2index\n",
    "index2phonem = vocab_phonem.index2word\n",
    "VOCAB_SIZE_PHONEM = len(index2phonem)\n",
    "\n",
    "# create embedding for phonems\n",
    "phonem_embedding_matrix = emb.get_phonem_embedding_matrix(phonem2index, VOCAB_SIZE_PHONEM)\n",
    "print(phonem_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    }
   ],
   "source": [
    "print(VOCAB_SIZE_PHONEM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation: Split sentences of text into data and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['yeah', 'you', 'know', 'what', 'this', 'is', 'nyc', ';', 'the', 'triumphant', 'return', 'rakim', 'allah', ';', 'rakim', ';'], 'remember'), (['you', 'know', 'what', 'this', 'is', 'nyc', ';', 'the', 'triumphant', 'return', 'rakim', 'allah', ';', 'rakim', ';', 'remember'], 'being'), (['know', 'what', 'this', 'is', 'nyc', ';', 'the', 'triumphant', 'return', 'rakim', 'allah', ';', 'rakim', ';', 'remember', 'being'], 'introduced'), (['what', 'this', 'is', 'nyc', ';', 'the', 'triumphant', 'return', 'rakim', 'allah', ';', 'rakim', ';', 'remember', 'being', 'introduced'], 'to'), (['this', 'is', 'nyc', ';', 'the', 'triumphant', 'return', 'rakim', 'allah', ';', 'rakim', ';', 'remember', 'being', 'introduced', 'to'], 'rapping')]\n"
     ]
    }
   ],
   "source": [
    "word_tokens = text.split()\n",
    "\n",
    "TIMESTEPS = 16\n",
    "\n",
    "str_data, str_labels = pre.create_data_label_pairs(word_tokens, TIMESTEPS)\n",
    "\n",
    "print( list( zip(str_data, str_labels) )[:5] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation: Convert previous data into phonems and keep the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update arpabet dictionary with unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "unknown_list = pre.get_text(\"data/cleaned-rap-lyrics/unknown_words_dict.txt\")\n",
    "unknown_dict = phon.create_unknown_dict_from_text(unknown_list)\n",
    "\n",
    "phon.update_arpabet(unknown_dict)\n",
    "print(len(phon.get_unknown_words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['Y', 'AE1', 'Y', 'UW1', 'N', 'OW1', 'W', 'AH1', 'T', 'DH', 'IH1', 'S', 'IH1', 'Z', 'N', 'IH', 'K', 'DH', 'AH0', 'T', 'R', 'AY0', 'AH1', 'M', 'F', 'AH0', 'N', 'T', 'R', 'IH0', 'T', 'ER1', 'N', 'R', 'AE', 'K', 'AH', 'M', 'AA1', 'L', 'AH0', 'R', 'AE', 'K', 'AH', 'M'], 'remember')]\n"
     ]
    }
   ],
   "source": [
    "phonem_data = []\n",
    "for sent in str_data:\n",
    "    phon_str = []\n",
    "    for word in sent:\n",
    "        if not word == \";\":\n",
    "            phon_str.extend(phon.get_phonem(word))\n",
    "    phonem_data.append(phon_str)\n",
    "    \n",
    "print( list( zip(phonem_data, str_labels) )[:1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words\n",
    "encoder = tr.IndexWordEncoder(\"Index-Word-Encoding\", word2index)\n",
    "decoder = tr.OneHotWordDecoder(\"1-Hot-Word-Decoding\", index2word, temperature=0.8)\n",
    "# phonem\n",
    "encoder_phonem = tr.IndexWordEncoder(\"Index-Phonem-Encoding\", phonem2index)\n",
    "\n",
    "data = encoder.encode(str_data)\n",
    "data_phonem = encoder_phonem.encode(phonem_data)\n",
    "labels = encoder.encode_labels(str_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete to save space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del str_labels\n",
    "del str_data\n",
    "del phonem_data\n",
    "del word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 16)\n",
      "(31, 35)\n",
      "(31, 66)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(labels.shape)\n",
    "print(data_phonem.shape)\n",
    "\n",
    "# choose timesteps according to length of padded phonem array\n",
    "TIMESTEPS_PHONEM = len(data_phonem[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Get seqlens of rap lyrics and phonem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED = True\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "embedding_dimension = 64\n",
    "embedding_dimension_phonem = 3\n",
    "\n",
    "hidden_layer_size = 32\n",
    "hidden_layer_size_phonem = 16\n",
    "\n",
    "num_LSTM_layers = 4\n",
    "num_LSTM_layers_phonem = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create input placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"embeddings\"):\n",
    "    if PRE_TRAINED:\n",
    "        embeddings = tf.Variable(tf.constant(0.0, shape=[VOCAB_SIZE, emb.GLOVE_SIZE]), trainable=True)\n",
    "        # If using pretrained embeddings, assign them to the embeddings variable\n",
    "        embedding_init = embeddings.assign(embedding_placeholder)\n",
    "        embed = tf.nn.embedding_lookup(embeddings, _inputs)\n",
    "    else:\n",
    "        embeddings = tf.Variable(tf.random_uniform([VOCAB_SIZE, embedding_dimension],\n",
    "                                                   -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, _inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phonem embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"embeddings_phonem\"):\n",
    "    if PRE_TRAINED:\n",
    "        embeddings_phonem = tf.Variable(tf.constant(0.0, shape=[VOCAB_SIZE_PHONEM, emb.PHONEM_SIZE]), trainable=True)\n",
    "        # If using pretrained embeddings, assign them to the embeddings variable\n",
    "        embedding_init_phonem = embeddings_phonem.assign(embedding_placeholder_phonem)\n",
    "        embed_phonem = tf.nn.embedding_lookup(embeddings_phonem, _inputs_phonem)\n",
    "    else:\n",
    "        embeddings_phonem = tf.Variable(tf.random_uniform([VOCAB_SIZE_PHONEM, embedding_dimension_phonem],\n",
    "                                                   -1.0, 1.0))\n",
    "        embed_phonem = tf.nn.embedding_lookup(embeddings_phonem, _inputs_phonem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"yeah you know what this is nyc ; the triumphant return rakim allah ; rakim ; remember being introduced to rapping your first rhyme ;\"\n",
    "sampler = lambda trainable, _: tr.sample(seed_text, trainable, encoder, decoder, length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/rene/workspace/Tensorflow/.pyenv/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from logs/10-test-glove-2/model\n",
      "Restoring an old model from 'logs/10-test-glove-2'\n"
     ]
    }
   ],
   "source": [
    "rnn_words = nn.MultiLayerRNN_2_embeddings(name=\"lstm-words-phonems\")\n",
    "rnn_words.build(num_LSTM_layers, hidden_layer_size, VOCAB_SIZE, TIMESTEPS, embedding_words=emb.GLOVE_SIZE, l2_reg=0.0\n",
    "                num_LSTM_layers_phonem, hidden_layer_size_phonem, VOCAB_SIZE_PHONEM, TIMESTEPS_PHONEM, embedding_phonems=emb.PHONEM_SIZE)\n",
    "\n",
    "tr.train_model(rnn_words, data, labels, sampler, epochs=EPOCHS, batch_size=BATCH_SIZE, log_dir=\"logs/10-test-glove-2\", retrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Sampling----------\n",
      "seed: \n",
      "killing people left and right \n",
      " use a gun cool homie \n",
      " that is right\n",
      "-\n",
      "result: \n",
      "killing people left and right \n",
      " use a gun cool homie \n",
      " that is right for effect ; scenery and mic words shallow ; giving to perfection to stomp a mic of it ; i stash you am i you you stop will not for this ; i know to heard to want to we came the crowd ; the world with because ya you\n"
     ]
    }
   ],
   "source": [
    "decoder.temperature = 0.7\n",
    "\n",
    "sampler = lambda trainable, seed_text: tr.sample( seed_text, trainable, encoder, decoder, length=50)\n",
    "sampler(rnn_words, \"killing people left and right \\n use a gun cool homie \\n that is right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith tf.variable_scope(\"lstm\"):\\n    # Define a function that gives the output in the right shape\\n    def lstm_cell():\\n        return tf.nn.rnn_cell.LSTMCell(hidden_layer_size, forget_bias=1.0)\\n    cell = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell() for _ in range(num_LSTM_layers)],\\n                                       state_is_tuple=True)\\n    outputs, states = tf.nn.dynamic_rnn(cell, embed,\\n                                        sequence_length=_seqlens,\\n                                        dtype=tf.float32)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    # Define a function that gives the output in the right shape\n",
    "    def lstm_cell():\n",
    "        return tf.nn.rnn_cell.LSTMCell(hidden_layer_size, forget_bias=1.0)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell() for _ in range(num_LSTM_layers)],\n",
    "                                       state_is_tuple=True)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                        sequence_length=_seqlens,\n",
    "                                        dtype=tf.float32)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM phonem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"lstm_phonem\"):\n",
    "    # Define a function that gives the output in the right shape\n",
    "    def lstm_cell_phonem():\n",
    "        return tf.nn.rnn_cell.LSTMCell(hidden_layer_size_phonem, forget_bias=1.0)\n",
    "    cell_phonem = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell_phonem() for _ in range(num_LSTM_layers_phonem)],\n",
    "                                       state_is_tuple=True)\n",
    "    outputs_phonem, states_phonem = tf.nn.dynamic_rnn(cell_phonem, embed_phonem,\n",
    "                                        sequence_length=_seqlens_phonem,\n",
    "                                        dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
