{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Predicting letter by letter with One-Hot-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    text = open(file_name, 'r').read()\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "class Alphabet:\n",
    "    def __init__(self, text):\n",
    "        from collections import Counter\n",
    "        self._count = Counter(list(text))\n",
    "        self._keys  = list(self._count.keys())\n",
    "        self._dict  = {}\n",
    "        for idx, key in enumerate(self._keys):\n",
    "            self._dict[key] = idx\n",
    "    \n",
    "    def get_count(self):\n",
    "        return self._count\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self._keys)\n",
    "    \n",
    "    def letter_to_index(self, letter):\n",
    "        return self._dict.get( letter, 'err' )\n",
    "    \n",
    "    def index_to_letter(self, index):\n",
    "        return self._keys[index]\n",
    "    \n",
    "    def one_hot(self, text):\n",
    "        encoded = []\n",
    "        for letter in text:\n",
    "            one_hot = [0] * self.get_size()\n",
    "            one_hot[self.letter_to_index(letter)] = 1\n",
    "            encoded.append(one_hot)\n",
    "        return np.array(encoded)\n",
    "    \n",
    "    def to_text(self, one_hots):\n",
    "        indices = np.argmax( one_hots, axis=1 ).tolist()\n",
    "        return \"\".join([self.index_to_letter(idx) for idx in indices])\n",
    "    \n",
    "    def indices_to_text(self, indices):\n",
    "        print(\"shape\")\n",
    "        print(indices.shape)\n",
    "        _indices = indices.tolist()\n",
    "        print(_indices)\n",
    "        return \"\".join([self.index_to_letter(idx) for idx in _indices])\n",
    "        \n",
    "    \n",
    "text = read_data(\"data/cleaned-rap-lyrics/clean2_pac_.txt\")\n",
    "alphabet = Alphabet(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unique characters: 37\n"
     ]
    }
   ],
   "source": [
    "print(f\"# unique characters: {alphabet.get_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = np.array(alphabet.one_hot(text))\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"as real as it seems the american dream\\nain't nothing but another calculated schemes\\nto get us locked\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet.to_text(encoded)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(num_data, batch_size):\n",
    "    \"\"\" Yield batches with indices until epoch is over.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_data: int\n",
    "        The number of samples in the dataset.\n",
    "    batch_size: int\n",
    "        The batch size used using training.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    batch_ixs: np.array of ints with shape [batch_size,]\n",
    "        Yields arrays of indices of size of the batch size until the epoch is over.\n",
    "    \"\"\"\n",
    "    \n",
    "    # data_ixs = np.random.permutation(np.arange(num_data))\n",
    "    data_ixs = np.arange(num_data)\n",
    "    ix = 0\n",
    "    while ix + batch_size < num_data:\n",
    "        batch_ixs = data_ixs[ix:ix+batch_size]\n",
    "        ix += batch_size\n",
    "        yield batch_ixs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification\n",
    "\n",
    "What we are building in this first instance is a 36-class classifier. Based on the previous characters, our model will predict the next upcoming character!\n",
    "\n",
    "#### Network Architecture:\n",
    "1. **RNN cell**: LSTM with hidden_layer_size\n",
    "2. **linear output layer**: maps to scores for 36 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(predicted, temperature=0.9):\n",
    "    '''\n",
    "     helper function to sample an index from a probability array\n",
    "     our model will output scores for each class\n",
    "     we normalize those outputs and create a probability distribution out of them to sample from\n",
    "    '''\n",
    "    exp_predicted = np.exp(predicted/temperature)\n",
    "    predicted = exp_predicted / np.sum(exp_predicted)\n",
    "    probabilities = np.random.multinomial(1, predicted, 1)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "    def build(self, hidden_layer_size, vocab_size, time_steps, l2_reg=0.0):\n",
    "        self.time_steps = time_steps\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape=[None, time_steps, vocab_size], name=\"data\")\n",
    "        self.Y = tf.placeholder(tf.int16, shape=[None, vocab_size], name=\"labels\")\n",
    "        \n",
    "        _X = tf.transpose(self.X, [1, 0, 2])\n",
    "        _X = tf.reshape(_X, [-1, vocab_size])\n",
    "        _X = tf.split(_X, time_steps, 0)\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            # 1x RNN LSTM Cell\n",
    "            self.rnn_cell   = tf.nn.rnn_cell.LSTMCell(hidden_layer_size)\n",
    "            \n",
    "            self.outputs, _ = tf.contrib.rnn.static_rnn(self.rnn_cell, _X, dtype=tf.float32)\n",
    "            \n",
    "            # 1x linear output layer\n",
    "            W_out = tf.Variable(tf.truncated_normal([hidden_layer_size, vocab_size], \n",
    "                                                 mean=0, stddev=.01))\n",
    "            b_out = tf.Variable(tf.truncated_normal([vocab_size],\n",
    "                                                mean=0, stddev=.01))\n",
    "            self.weights.append(W_out)\n",
    "            self.biases.append(b_out)\n",
    "            \n",
    "            self.last_rnn_output = self.outputs[-1]\n",
    "            self.final_output    = self.last_rnn_output @ W_out + b_out\n",
    "            \n",
    "            # softmax cross entropy as our loss function (between 36 classes)\n",
    "            self.softmax = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.final_output,\n",
    "                                                                labels=self.Y)\n",
    "            self.cross_entropy_loss = tf.reduce_mean(self.softmax)\n",
    "            \n",
    "            self.loss = self.cross_entropy_loss\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer()\n",
    "            self.train_step= self.optimizer.minimize(self.loss)\n",
    "            \n",
    "            self.correct_prediction = tf.equal(tf.argmax(self.Y,1), tf.argmax(self.final_output, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))*100\n",
    "    \n",
    "    def train(self, train_data, train_labels, alphabet, epochs=20, batch_size=128):\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        \n",
    "        self.session = tf.Session()\n",
    "        session = self.session\n",
    "        \n",
    "        with session.as_default():\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            tr_loss, tr_acc = session.run([self.loss, self.accuracy],\n",
    "                                          feed_dict={self.X: train_data,\n",
    "                                                     self.Y: train_labels})\n",
    "            train_losses.append(tr_loss)\n",
    "            train_accs.append(tr_acc)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                \n",
    "                if(epoch + 1) % 1 == 0:\n",
    "                    print(f\"\\n\\nEpoch {epoch + 1}/{epochs}\")\n",
    "                    print(f\"Loss:    \\t {tr_loss}\")\n",
    "                    print(f\"Accuracy:\\t {tr_acc}\")\n",
    "                \n",
    "                for batch_ixs in batch_data(len(train_data), batch_size):\n",
    "                    _ = session.run(self.train_step,\n",
    "                                   feed_dict={\n",
    "                                       self.X: train_data[batch_ixs],\n",
    "                                       self.Y: train_labels[batch_ixs],\n",
    "                                   })\n",
    "                tr_loss, tr_acc = session.run([self.loss, self.accuracy],\n",
    "                                               feed_dict={self.X: train_data,\n",
    "                                                          self.Y: train_labels\n",
    "                                                         })\n",
    "                train_losses.append(tr_loss)\n",
    "                train_accs.append(tr_acc)\n",
    "                \n",
    "                #get on of training set as seed\n",
    "                seed = train_data[:1:]\n",
    "        \n",
    "                #to print the seed 40 characters\n",
    "                seed_chars = ''\n",
    "                for each in seed[0]:\n",
    "                    seed_chars += alphabet._keys[np.where(each == max(each))[0][0]]\n",
    "                print (\"Seed:\" + seed_chars)\n",
    "        \n",
    "                #predict next 500 characters\n",
    "                for i in range(500):\n",
    "                    if i > 0:\n",
    "                        remove_fist_char = seed[:,1:,:]\n",
    "                        seed = np.append(remove_fist_char, np.reshape(probabilities, [1, 1, self.vocab_size]), axis=1)\n",
    "                        \n",
    "                    predicted = session.run([self.final_output], feed_dict = {self.X:seed})\n",
    "                    predicted = np.asarray(predicted[0]).astype('float64')[0]\n",
    "                    probabilities = sample(predicted)\n",
    "                    predicted_chars = alphabet._keys[np.argmax(probabilities)]\n",
    "                    seed_chars += predicted_chars\n",
    "                print ('Result:'+ seed_chars)\n",
    "        \n",
    "        self.hist = {\n",
    "            'train_losses': np.array(train_losses),\n",
    "            'train_accuracy': np.array(train_accs)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_data('data/cleaned-rap-lyrics/clean2_pac_.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "\n",
    "HIDDEN = 256\n",
    "VOCAB_SIZE = 37\n",
    "TIME_STEPS = 20\n",
    "EPOCHS = 20\n",
    "\n",
    "def making_one_hot(text, alphabet):\n",
    "    '''\n",
    "    '''\n",
    "    unique_chars = alphabet._keys\n",
    "    len_unique_chars = len(unique_chars)\n",
    "\n",
    "    input_chars = []\n",
    "    output_char = []\n",
    "    \n",
    "    for i in range(0, len(text) - TIME_STEPS, step):\n",
    "        input_chars.append(text[i:i+TIME_STEPS])\n",
    "        output_char.append(text[i+TIME_STEPS])\n",
    "    train_data = np.zeros((len(input_chars), TIME_STEPS, len_unique_chars))\n",
    "    target_data = np.zeros((len(input_chars), len_unique_chars))\n",
    "    for i , each in enumerate(input_chars):\n",
    "        for j, char in enumerate(each):\n",
    "            train_data[i, j, unique_chars.index(char)] = 1\n",
    "        target_data[i, unique_chars.index(output_char[i])] = 1\n",
    "    return train_data, target_data, unique_chars, len_unique_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data, tr_labels, unique_chars, len_unique = making_one_hot(text, alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1/20\n",
      "Loss:    \t 3.612441062927246\n",
      "Accuracy:\t 1.9086482524871826\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems oont pon nyk e i ltnnt ind nithonnt t lokfa'\n",
      "dae' posnuhd nu ehmrluse y'w witi ao tre s yov inn cot to ieiec rane nhns'e\n",
      "zrd lnomi ioa tu tis\n",
      "he wabn wonl khe yhe theg\n",
      "wilm anl edt\n",
      "folgtathe komn yit oon tiw tocle t me wwist lorhi eon you cfue theb euil icpei wime wig ine rit arlnk bonr aonpre ud cheim finp iuei ih n'r tnl lolan wheptneotm\n",
      "ai k \n",
      "an toafe hhl'on me mee tat tncvqhabine  ony go kimd gmo iy 'tde yockn koun\n",
      "\n",
      "arh d wanup tuiv nee felegth tn the ayad breekas nounocet n thw fen worne\n",
      "y \n",
      "\n",
      "\n",
      "Epoch 2/20\n",
      "Loss:    \t 2.5963056087493896\n",
      "Accuracy:\t 26.289981842041016\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems ce led ad ufe sow bomecrach toi sawme tocd dhing y merel ay ihe ere mepd nin it war san whe foul\n",
      "the edlt mith hfet nu dike nok prikhectow weid oy the ine awebe cled thit tom the sonane goreot cengen krent tuwy uo' ihin saon be in iss ale an't srok mem onic'r\n",
      "aan ir und yao' the tid fom foukn tn chic thou hes at win ou ca's e thoy critne br eona layen lftt tromes\n",
      "tomd wink iid arxther\n",
      "ning ig' eannh thet thee ec yawat fote ane lum srsingric dule cindt nn hhas y cotin wyy dee thy sig'ule more'g w\n",
      "\n",
      "\n",
      "Epoch 3/20\n",
      "Loss:    \t 2.3774466514587402\n",
      "Accuracy:\t 30.76872444152832\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems whoz the tope o ipa bigne yor cony hone wackend busir you sesrou din' to bulqneyer ity git you sonfye tougo gox ine witi oot banith 'ot walfs\n",
      "but's i' klre and wanl now toe therithe coak mut tol cewe ton thet mide iz\n",
      "in' thet warl theas int in' bunted tof meth ckin towtle buth ne sheu got wakn cute woalvelt't onin monnm reinin\n",
      "thon br bost\n",
      "sat in firng thite\n",
      "toun  if\n",
      "the ceves fuo slon tour cemex avi'n'n therek wale \n",
      "capve\n",
      "nafe\n",
      "ie wbmed\n",
      "ig touss fo\n",
      "trey fack ond coum ny budld col's ang an th the\n",
      "\n",
      "\n",
      "Epoch 4/20\n",
      "Loss:    \t 2.2713520526885986\n",
      "Accuracy:\t 32.721797943115234\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems ge\n",
      "i ion woog the eolethein\n",
      "the aniy for acr rot motyereceragey waps caond in\n",
      "tre thee in bulk brmariln wee t oe bad ind as yot i ge traole shon the pyorrer buck they therd the\n",
      "sragk mewhir meh me\n",
      "i' you buckin bonan't \n",
      "at thele wonkine und seme thot i namd i'pr anle ind to\n",
      "etind thetvlickem\n",
      "be higg\n",
      "pe wot in't i'l thrrin't bumcfunl i gime\n",
      "cyopl fop ane crcyide then mist wemyd\n",
      "ohe non'w i cosgen\n",
      "do her unk i'p ge nick verlave notiagoa tot andine uo't ryome\n",
      "thitk\n",
      "prest up sine i' zrivh ferlher a \n",
      "\n",
      "\n",
      "Epoch 5/20\n",
      "Loss:    \t 2.1866016387939453\n",
      "Accuracy:\t 35.051666259765625\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems a't sigete you caming\n",
      "hepomd mo oun' a'd biver int and uf amal ove some yrus trand on foke\n",
      "thrreut and as desuo tralk my weon irull my binf you won stro gemich the gend gow it pume juty furver\n",
      "het i's ingon'r ow\n",
      "sthe neod fela te chips pakin gu ley wat nom ind anit\n",
      "the wher rafes and nrove nigby fumo\n",
      "thet i chamen inou't o goclin' fot niggy\n",
      "non't thit ge weon coolin' st me hems angole on ther sounsin' and't uut whlegl thing yoy het got a souninot beckin the tom cuss hike wenmanl on nok mer quy n\n",
      "\n",
      "\n",
      "Epoch 6/20\n",
      "Loss:    \t 2.1125717163085938\n",
      "Accuracy:\t 36.695404052734375\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems sucked gos i douar\n",
      "ars'tad a muthe\n",
      "tha'e thend you pravune thet gut stoed thind and quake nimad o donoilanse's  han the ind my is hi'n got thin taut or hathlas hick\n",
      "un then now they they and whowjoppin ur thick of the fee oof wo head band tor i'm nicking the nowas i gith they thipn'\n",
      "could con bemn a't\n",
      "to coucg it toonli got like ovam micr as mo in\n",
      "get me\n",
      "heant hom rifuni when wist to ju\n",
      "stees at gut's toucken woy on they stowe cond i cropper ut son to cinh\n",
      "tokrare s'll comin't toug couck igl don\n",
      "\n",
      "\n",
      "Epoch 7/20\n",
      "Loss:    \t 2.0521016120910645\n",
      "Accuracy:\t 38.68632507324219\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems get coust  kicn\n",
      "they onx it the kine butt githen nan i weos out um buoking gett\n",
      "you cam wire got in uok and ef wiln you head doabbund secupe\n",
      "hit so bant in anl thet slobe cremin' looe got i teas leant\n",
      "fer an wull\n",
      "whan soers ne beam wind track what stle tald whe fism\n",
      "of newin i't\n",
      "camys\n",
      "wat everpesuin\n",
      "the matofroune anked eokeruncliy gat arssyoke and inw\n",
      "shequs\n",
      "in the fnow thy kntopind go chis somas\n",
      "you barts\n",
      "gom me cruekin tom then as waria the dock can\n",
      "hem iswis\n",
      "bumne\n",
      "lend all wotne fuck nik\n",
      "not\n",
      "\n",
      "\n",
      "Epoch 8/20\n",
      "Loss:    \t 2.0015344619750977\n",
      "Accuracy:\t 40.010860443115234\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems you' kiof of and at paln at\n",
      "the wack shot i'm lock rotlee in't wive sale thang brade coucl ack oz shind it cazn\n",
      "not caup\n",
      "nats to couse dous\n",
      "hallse to tide with\n",
      "niget i geveo if he donne bord on that to to pingtay it\n",
      "you't ther you lall ther and peat whaser me to me\n",
      "prowe litter stuthentet\n",
      "in the\n",
      "ion now und th fuck my\n",
      "ann that stomp\n",
      "whyt cham shew in anl sictthen my domuthen githin\n",
      "ars donit\n",
      "sfeen 2pryign\n",
      "i'm and pralin' but carled i caect botho loknown\n",
      "the nomen yor cried juck now crien rortali\n",
      "\n",
      "\n",
      "Epoch 9/20\n",
      "Loss:    \t 1.958274245262146\n",
      "Accuracy:\t 41.147823333740234\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems like feolin pay hat yous cecat\n",
      "thiy knop und me bropitt\n",
      "me backed bick\n",
      "shack git juck rome and 2lele wead wotke now't you getr ppang n'llice to one \n",
      "then thon't wrenging that\n",
      "i glane my pecesmtryich rowice i'm shepl whoke i gotsenf and of hend wacki's will brop but tree it cause so ala\n",
      "you't buttin tot dond anlezin a toumecony\n",
      "tot you'll dote to make\n",
      "like of mery\n",
      "promes hith sicki'm gurse\n",
      "i foring thoo weops like\n",
      "the booch now and feok seaypelme then got when the paldin ave ore geod to gould\n",
      "you\n",
      "\n",
      "\n",
      "Epoch 10/20\n",
      "Loss:    \t 1.9187699556350708\n",
      "Accuracy:\t 42.166316986083984\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems an i'm witp aver kean the eim to mikin' nom deach and cackes\n",
      "thee caste acfow you'll lere hetca i't don't crucked up make cimn you' got wan add ill alkin' i burl you kitt\n",
      "i thing copne i powin'm fuck dao thes with\n",
      "to to the fodbever comin\n",
      "bimlleded brom whoke be thisged rettit\n",
      "tomen get jasin'  kind wa tore got now my ritin yer\n",
      "inc i klet in thar op troway\n",
      "hly i'm theed puckthon whap\n",
      "keal i'm ghep croppens get ont bepcracked up\n",
      "you forse they ambanin theo hit ay wastin' lime ith hit she my and '\n",
      "\n",
      "\n",
      "Epoch 11/20\n",
      "Loss:    \t 1.8818473815917969\n",
      "Accuracy:\t 43.375675201416016\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems wald an\n",
      "the hat my deack iss andyserarline i tom tho lemy whot sliek\n",
      "thele mepmerout\n",
      "me gotein comet\n",
      "and with a mosin't swarts yow or stitelins deats ming line you crouped thatly ore hom the barin anarodrick\n",
      "i's a cauyen in forhed on i watse whin't beabi\n",
      "geve my lack outh sfat\n",
      "thing then marin we me cancona metin toude far libe mo goreets\n",
      "came\n",
      "in cropped what bat wotter the fore and so\n",
      "wenta to git nigga if lack one peadne and coust cone bats\n",
      "the fann my flotten dop lot\n",
      "this the tos me\n",
      "but vill \n",
      "\n",
      "\n",
      "Epoch 12/20\n",
      "Loss:    \t 1.8513853549957275\n",
      "Accuracy:\t 43.94826889038086\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems got tho ming i'l younda love me sondevonguce seocker jath wulk alk us nopp\n",
      "to be a welly to beath robearo thend on\n",
      "propledop\n",
      "cromanion they aid he winkel now i'm dimerast't tot be brieve\n",
      "the somend chalk but weyt wo ker beopsl mon\n",
      "i aase i met i wimes and ollend but whit the nigeted ticking stacked cotming thell paydosthow shepphauglad it triigen the niinin all is heeps\n",
      "to ckism\n",
      "he beac thit theped one litnod\n",
      "ies that goted ays with thanta sooth likin bet i fell of wanna mebpack they whot a now \n",
      "\n",
      "\n",
      "Epoch 13/20\n",
      "Loss:    \t 1.8167322874069214\n",
      "Accuracy:\t 44.78412628173828\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems it like\n",
      "up way winther comece to ther overese with with and the mod to the fotin it you dofe begaty aits to then got yo heare my mother droug a stee sutine you mever biss and if getted con tothind it licento to dome\n",
      "whel in you gevin''t to cucked feep tires on the t i's a'm dackly\n",
      "it and so i'm mebae pook yow come c is of shioked\n",
      "to peed fook on thiout to chick you knows\n",
      "a cand\n",
      "henk and find ongas it whad solright gow you's thin leove you goy un bettend thaod nowed fo'll in\n",
      "my beay 2pomin to to \n",
      "\n",
      "\n",
      "Epoch 14/20\n",
      "Loss:    \t 1.7890236377716064\n",
      "Accuracy:\t 45.67757034301758\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems they i'm how i fot that sioch alm and gove wank niggal se it thing me dack out\n",
      "to never kex\n",
      "now of t'min wotte i solo canse they pack a grazs brinbate\n",
      "to coure i'll mad bots like a collomaver it wes mans\n",
      "i'm fore evinne dome ense i legever ther the tripps fownd gigethace in you med whackin 2pict wall in the ant on whing toull coull you' to the bune it's hor four to faig well me criminal collend to came comis don't grose bot a coure to hill behon\n",
      "breacis fest tay\n",
      "cutming now my wink to me\n",
      "by and \n",
      "\n",
      "\n",
      "Epoch 15/20\n",
      "Loss:    \t 1.7638659477233887\n",
      "Accuracy:\t 46.317626953125\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems of now dueres kind and so my all hoves crimind\n",
      "but in this is as with noworecory hive to the furked avery thin mence for the bropled yow a nowin in time me to being you cane with ne's of leall\n",
      "buy solmiction\n",
      "yo 'moo compend yo ruckin a way he chiminap tored\n",
      "i bus loke froms they call thing token but the picker dop\n",
      "the got if fer thim ain the beet mooked they seect cimpedin illated with the goms\n",
      "mack to faget a futser wath need of the willow\n",
      "buil\n",
      "thi got thit it shoo with miss i got's ronnice\n",
      "bam\n",
      "\n",
      "\n",
      "Epoch 16/20\n",
      "Loss:    \t 1.7386630773544312\n",
      "Accuracy:\t 46.957679748535156\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems crosped it pund wasea you's a cho'se surpin betoulin to batin to deed he tryons is or whind you was hit they gray and get you wan't clowmin'\n",
      "to lime all it'm thay then thaw yow your on\n",
      "barad af how the know\n",
      "you got know ropped of ant schopge for came up wither be to 'pudin through the chatin ming mind on the shaken nime\n",
      "i'm a mefed i'm methinger soully hinga aon'toked you hid that champ drop sicker h we aly ayoull your get dostane ictomy to stroind i can cause it nast\n",
      "i wean\n",
      "that show be kitat m\n",
      "\n",
      "\n",
      "Epoch 17/20\n",
      "Loss:    \t 1.7146018743515015\n",
      "Accuracy:\t 47.44636154174805\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems in the lotime ning and wetthelfuchin the wraty acrack tome too fupped to soull of the beec\n",
      "bot i fut the fun trapped up the bit to dees subbed when you hall\n",
      "the micrice poothe with the lead in the say\n",
      "nigeao\n",
      "but me\n",
      "it id a pooden\n",
      "witna get live\n",
      "not and dazn for gonddian thimg blanknn\n",
      "gill to fin got coust efpiniw you life aim mo fadnc niggaz\n",
      "ffeed watch chast on\n",
      "smebpicniend\n",
      "i comised to andy nowe\n",
      "i wan menin a bat duck my stards up and be crupped to sheakin ays and a fouss i with\n",
      "to get theyen\n",
      "\n",
      "\n",
      "\n",
      "Epoch 18/20\n",
      "Loss:    \t 1.6914764642715454\n",
      "Accuracy:\t 48.14071273803711\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems all you could it wasn't commtast\n",
      "whin they with in the way lobed for puck inget\n",
      "bmendy fone let jublin' shated\n",
      "to me beny gonna by fabnin' apred\n",
      "prooned ap in the dove enatitiour\n",
      "dien pillin ghat i copl betin with her coms funnt to kowaine ithat don't shoot howt soo shi'll you lanise strive i knew be bristhin the praie an smewing netem wollen't botem to see that me it tome now\n",
      "i'm trigst is this\n",
      "the wainh on the ziggaz\n",
      "with the thinky that crowe to me\n",
      "poldice\n",
      "is\n",
      "eruminin thood he's  lowe and i g\n",
      "\n",
      "\n",
      "Epoch 19/20\n",
      "Loss:    \t 1.6691274642944336\n",
      "Accuracy:\t 48.828487396240234\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems and i'm cloppea' tripin math\n",
      "came nane way clowe and andingacan whin the stop quick\n",
      "broblend\n",
      "and but eme it like\n",
      "pon't mother we dought to the onothe\n",
      "ont voucy\n",
      "to of to be brephed\n",
      "with the sell wime at be the shit\n",
      "we fack mone well all yous with\n",
      "rope\n",
      "nov right toked and dould i lay and a you go fod mon\n",
      "the hitseas\n",
      "gon a bat\n",
      "i see i way but don't this get my buttin crimen' mo boter in out sollike to inde with the fexped\n",
      "chiminal betting blanc and pillin  souljty we beat on walk in the wall chow t\n",
      "\n",
      "\n",
      "Epoch 20/20\n",
      "Loss:    \t 1.6472580432891846\n",
      "Accuracy:\t 49.46360397338867\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems willed to dene\n",
      "cares and beave\n",
      "you kep soun\n",
      "the douldn't wen the care done\n",
      "on the beaz\n",
      "thoy way walls than he treppes man furgs who still might fore a pad fome quink one bloke you wayn't motco you run i fun\n",
      "stack\n",
      "who doanted to get to i'd getton how degome with my down\n",
      "bringave live of the ints dexe\n",
      "you liel her do now they got him unmon\n",
      "i'm casse yous\n",
      "hehavi's that way on no loot comenes entodin get way tright all i'm sex brimi olese i'm thes me the botine drop\n",
      "sick\n",
      "of equiess that i chaie i'm \n"
     ]
    }
   ],
   "source": [
    "basicRNN = RNN(name = \"basic\")\n",
    "basicRNN.build(HIDDEN, VOCAB_SIZE, TIME_STEPS)\n",
    "basicRNN.train(tr_data, tr_labels, alphabet, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learnings\n",
    "Some of words slowly begin to make sense, but it is mostly gibberish. Instead of predicting letters, we should try to predict word by word!\n",
    "\n",
    "Have a look at **02-word-embedding** for that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
