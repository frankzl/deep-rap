{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Predicting letter by letter with One-Hot-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    text = open(file_name, 'r').read()\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "class Alphabet:\n",
    "    def __init__(self, text):\n",
    "        from collections import Counter\n",
    "        self._count = Counter(list(text))\n",
    "        self._keys  = list(self._count.keys())\n",
    "        self._dict  = {}\n",
    "        for idx, key in enumerate(self._keys):\n",
    "            self._dict[key] = idx\n",
    "    \n",
    "    def get_count(self):\n",
    "        return self._count\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self._keys)\n",
    "    \n",
    "    def letter_to_index(self, letter):\n",
    "        return self._dict.get( letter, 'err' )\n",
    "    \n",
    "    def index_to_letter(self, index):\n",
    "        return self._keys[index]\n",
    "    \n",
    "    def one_hot(self, text):\n",
    "        encoded = []\n",
    "        for letter in text:\n",
    "            one_hot = [0] * self.get_size()\n",
    "            one_hot[self.letter_to_index(letter)] = 1\n",
    "            encoded.append(one_hot)\n",
    "        return np.array(encoded)\n",
    "    \n",
    "    def to_text(self, one_hots):\n",
    "        indices = np.argmax( one_hots, axis=1 ).tolist()\n",
    "        return \"\".join([self.index_to_letter(idx) for idx in indices])\n",
    "    \n",
    "    def indices_to_text(self, indices):\n",
    "        print(\"shape\")\n",
    "        print(indices.shape)\n",
    "        _indices = indices.tolist()\n",
    "        print(_indices)\n",
    "        return \"\".join([self.index_to_letter(idx) for idx in _indices])\n",
    "        \n",
    "    \n",
    "text = read_data(\"data/cleaned-rap-lyrics/clean2_pac_.txt\")\n",
    "alphabet = Alphabet(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unique characters: 37\n"
     ]
    }
   ],
   "source": [
    "print(f\"# unique characters: {alphabet.get_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = np.array(alphabet.one_hot(text))\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"as real as it seems the american dream\\nain't nothing but another calculated schemes\\nto get us locked\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet.to_text(encoded)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(num_data, batch_size):\n",
    "    \"\"\" Yield batches with indices until epoch is over.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_data: int\n",
    "        The number of samples in the dataset.\n",
    "    batch_size: int\n",
    "        The batch size used using training.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    batch_ixs: np.array of ints with shape [batch_size,]\n",
    "        Yields arrays of indices of size of the batch size until the epoch is over.\n",
    "    \"\"\"\n",
    "    \n",
    "    # data_ixs = np.random.permutation(np.arange(num_data))\n",
    "    data_ixs = np.arange(num_data)\n",
    "    ix = 0\n",
    "    while ix + batch_size < num_data:\n",
    "        batch_ixs = data_ixs[ix:ix+batch_size]\n",
    "        ix += batch_size\n",
    "        yield batch_ixs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification\n",
    "\n",
    "What we are building in this first instance is a 36-class classifier. Based on the previous characters, our model will predict the next upcoming character!\n",
    "\n",
    "#### Network Architecture:\n",
    "1. **RNN cell**: LSTM with hidden_layer_size\n",
    "2. **linear output layer**: maps to scores for 36 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(predicted, temperature=0.9):\n",
    "    '''\n",
    "     helper function to sample an index from a probability array\n",
    "     our model will output scores for each class\n",
    "     we normalize those outputs and create a probability distribution out of them to sample from\n",
    "    '''\n",
    "    exp_predicted = np.exp(predicted/temperature)\n",
    "    predicted = exp_predicted / np.sum(exp_predicted)\n",
    "    probabilities = np.random.multinomial(1, predicted, 1)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "    def build(self, hidden_layer_size, vocab_size, time_steps, l2_reg=0.0):\n",
    "        self.time_steps = time_steps\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape=[None, time_steps, vocab_size], name=\"data\")\n",
    "        self.Y = tf.placeholder(tf.int16, shape=[None, vocab_size], name=\"labels\")\n",
    "        \n",
    "        _X = tf.transpose(self.X, [1, 0, 2])\n",
    "        _X = tf.reshape(_X, [-1, vocab_size])\n",
    "        _X = tf.split(_X, time_steps, 0)\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            # 1x RNN LSTM Cell\n",
    "            self.rnn_cell   = tf.nn.rnn_cell.LSTMCell(hidden_layer_size)\n",
    "            \n",
    "            self.outputs, _ = tf.contrib.rnn.static_rnn(self.rnn_cell, _X, dtype=tf.float32)\n",
    "            \n",
    "            # 1x linear output layer\n",
    "            W_out = tf.Variable(tf.truncated_normal([hidden_layer_size, vocab_size], \n",
    "                                                 mean=0, stddev=.01))\n",
    "            b_out = tf.Variable(tf.truncated_normal([vocab_size],\n",
    "                                                mean=0, stddev=.01))\n",
    "            self.weights.append(W_out)\n",
    "            self.biases.append(b_out)\n",
    "            \n",
    "            self.last_rnn_output = self.outputs[-1]\n",
    "            self.final_output    = self.last_rnn_output @ W_out + b_out\n",
    "            \n",
    "            # softmax cross entropy as our loss function (between 36 classes)\n",
    "            self.softmax = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.final_output,\n",
    "                                                                labels=self.Y)\n",
    "            self.cross_entropy_loss = tf.reduce_mean(self.softmax)\n",
    "            \n",
    "            self.loss = self.cross_entropy_loss\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer()\n",
    "            self.train_step= self.optimizer.minimize(self.loss)\n",
    "            \n",
    "            self.correct_prediction = tf.equal(tf.argmax(self.Y,1), tf.argmax(self.final_output, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))*100\n",
    "    \n",
    "    def train(self, train_data, train_labels, alphabet, epochs=20, batch_size=128):\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        \n",
    "        self.session = tf.Session()\n",
    "        session = self.session\n",
    "        \n",
    "        with session.as_default():\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            tr_loss, tr_acc = session.run([self.loss, self.accuracy],\n",
    "                                          feed_dict={self.X: train_data,\n",
    "                                                     self.Y: train_labels})\n",
    "            train_losses.append(tr_loss)\n",
    "            train_accs.append(tr_acc)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                \n",
    "                if(epoch + 1) % 1 == 0:\n",
    "                    print(f\"\\n\\nEpoch {epoch + 1}/{epochs}\")\n",
    "                    print(f\"Loss:    \\t {tr_loss}\")\n",
    "                    print(f\"Accuracy:\\t {tr_acc}\")\n",
    "                \n",
    "                for batch_ixs in batch_data(len(train_data), batch_size):\n",
    "                    _ = session.run(self.train_step,\n",
    "                                   feed_dict={\n",
    "                                       self.X: train_data[batch_ixs],\n",
    "                                       self.Y: train_labels[batch_ixs],\n",
    "                                   })\n",
    "                tr_loss, tr_acc = session.run([self.loss, self.accuracy],\n",
    "                                               feed_dict={self.X: train_data,\n",
    "                                                          self.Y: train_labels\n",
    "                                                         })\n",
    "                train_losses.append(tr_loss)\n",
    "                train_accs.append(tr_acc)\n",
    "                \n",
    "                #get on of training set as seed\n",
    "                seed = train_data[:1:]\n",
    "        \n",
    "                #to print the seed 40 characters\n",
    "                seed_chars = ''\n",
    "                for each in seed[0]:\n",
    "                    seed_chars += alphabet._keys[np.where(each == max(each))[0][0]]\n",
    "                print (\"Seed:\" + seed_chars)\n",
    "        \n",
    "                #predict next 500 characters\n",
    "                for i in range(500):\n",
    "                    if i > 0:\n",
    "                        remove_fist_char = seed[:,1:,:]\n",
    "                        seed = np.append(remove_fist_char, np.reshape(probabilities, [1, 1, self.vocab_size]), axis=1)\n",
    "                        \n",
    "                    predicted = session.run([self.final_output], feed_dict = {self.X:seed})\n",
    "                    predicted = np.asarray(predicted[0]).astype('float64')[0]\n",
    "                    probabilities = sample(predicted)\n",
    "                    predicted_chars = alphabet._keys[np.argmax(probabilities)]\n",
    "                    seed_chars += predicted_chars\n",
    "                print ('Result:'+ seed_chars)\n",
    "        \n",
    "        self.hist = {\n",
    "            'train_losses': np.array(train_losses),\n",
    "            'train_accuracy': np.array(train_accs)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_data('data/cleaned-rap-lyrics/clean2_pac_.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "\n",
    "HIDDEN = 128\n",
    "VOCAB_SIZE = 37\n",
    "TIME_STEPS = 20\n",
    "EPOCHS = 10\n",
    "\n",
    "def making_one_hot(text, alphabet):\n",
    "    '''\n",
    "    '''\n",
    "    unique_chars = alphabet._keys\n",
    "    len_unique_chars = len(unique_chars)\n",
    "\n",
    "    input_chars = []\n",
    "    output_char = []\n",
    "    \n",
    "    for i in range(0, len(text) - TIME_STEPS, step):\n",
    "        input_chars.append(text[i:i+TIME_STEPS])\n",
    "        output_char.append(text[i+TIME_STEPS])\n",
    "    train_data = np.zeros((len(input_chars), TIME_STEPS, len_unique_chars))\n",
    "    target_data = np.zeros((len(input_chars), len_unique_chars))\n",
    "    for i , each in enumerate(input_chars):\n",
    "        for j, char in enumerate(each):\n",
    "            train_data[i, j, unique_chars.index(char)] = 1\n",
    "        target_data[i, unique_chars.index(output_char[i])] = 1\n",
    "    return train_data, target_data, unique_chars, len_unique_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data, tr_labels, unique_chars, len_unique = making_one_hot(text, alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1/10\n",
      "Loss:    \t 3.6113319396972656\n",
      "Accuracy:\t 5.893774032592773\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems mte  mwe  ie  moom iia m spmorewiacs\n",
      "feso\n",
      "ee ii\n",
      "ti se tdt a'paiar we tcw nedsgeto f csgedol   netde eeth' heey iu re h\n",
      "nr adt entndeoe kee linod shpi'l r oyikse ajle\n",
      "\n",
      "fe tstttond   bh noou tha tip  kaa ijog aibel\n",
      "cutkrks cbownaa\n",
      "'e\n",
      "nlnzi usk s wrsett  wee  fueo niny dhoh\n",
      "2g\n",
      " ogie ws t a ne vehggvttvt h'o nhculpsy wgwheat eim ftahetwr gkttt no r nptot  mertalosit stk cofwl shatereh ncnnnea ahs oio\n",
      "fjlide rh c var ee lzo\n",
      "toi romiand alnnt ten b btheuadst ellgnstow  uahn yerl'lwol iaiilte etbs  p\n",
      "o\n",
      "\n",
      "\n",
      "Epoch 2/10\n",
      "Loss:    \t 2.7001945972442627\n",
      "Accuracy:\t 26.048110961914062\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems cifl bninos ak mya zi herad wane joyo anlith woiw hat man' lats y inew i ffi'  itkez'n cith t'utwritee if lrsrdtgtonn bewyd me waupatide tou loi thirk toucl ms eos il tacd gonf mo laye whene thit keret of hit ou  astha af on'd 'u f yome nout mapea\n",
      "ton tov go tfak rom shonk\n",
      "notogd\n",
      "asapd ndn so kid in''\n",
      " inhint tho sn cyuzlt net bon merlyees couh ygly amathe g fu nonrel\n",
      "fasthsit don' bian 'd whe mert y y ouut ibnle azew bcone soe wind sed edel necale faothe lpz say mat mer the nain lar'uep qucle y\n",
      "\n",
      "\n",
      "Epoch 3/10\n",
      "Loss:    \t 2.432081460952759\n",
      "Accuracy:\t 29.992103576660156\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems rucked sfren wakzor wu brvat thlavel bact thit' far fut wus shink t alk therin'l chobit\n",
      "gal\n",
      "the the dodethir\n",
      "pure ron bumd hith wak s watc al cain' thet wak rhanim thet ind jahe yim the b thon s iut hope\n",
      "feseef inl seoci hom img wopnrciks toat yock wer whinlef fuck yo inl gerd nenisgeol tou bume yy back ox wel nole ild bad gon erydanl vankey lafle\n",
      "taflens ris'fe hei's gokl cnmened gh ther be ild  andt lasthe be nhag go buc't is is ou' i whe waspar tild areen to ansriicl ya't bost fonge hotley bu\n",
      "\n",
      "\n",
      "Epoch 4/10\n",
      "Loss:    \t 2.328226327896118\n",
      "Accuracy:\t 31.152099609375\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems mame thyy her pome saut rocelane batths cus gits the ina slack tre asme butgan in bon ictath so gam onveo thit in wnamd cheore youngox to slethe ao hi hat lafke nas candt on't rot tos nol coulkno shot wotheu cecaest cumthy' un jusel've nhy neckin's the chiwet wpey ar get weathe't wall yaat n mo gither to lilpe so oll heve ize fore sevifcend ti khebnen thins\n",
      "wims tist are pafeng\n",
      "in's frallint thes ind gis eathn yha buck basge spine hoke dom bockty\n",
      "ifelll witp anoy sourfe ime\n",
      "aute suteh'th oos and\n",
      "\n",
      "\n",
      "Epoch 5/10\n",
      "Loss:    \t 2.2538466453552246\n",
      "Accuracy:\t 33.43589782714844\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems thowp\n",
      "fuge lame five rios yous aco hif the whi hicn\n",
      "to thon lyouine nrove wotthey withik coun'thatherulls heone's the gow ber seom ond ind ill eoge gome geschot bap anw nean't wipes hea' the beuky hond not whathe heet\n",
      "tha slut by weilibo atyen the nsto\n",
      "ctry coanst wotthy nowetefe ouve fots of bear ous nomigm to kuts nit wotir unget me couitrethe tand on tran thiy souigerlr it hot get goft tuakve wawh wrat arcall\n",
      "fhite ghe soucit fand th me civer heom tom fut the lade ypo the i nup tome ina d ong\n",
      "\n",
      "\n",
      "Epoch 6/10\n",
      "Loss:    \t 2.194096088409424\n",
      "Accuracy:\t 35.16190719604492\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems thacke anothy hicltst ind of whos atund bote at aelllllsteas sick soor butk nick wuth nowes im weing he when t agery fop home gay'\n",
      "tine i'f no sfowinf toot'st wean buck\n",
      "hof food at\n",
      "at d the bike\n",
      "son toy nomd fezteif\n",
      "i'l crut nead angat to the pucktin'w whane my yhus dred\n",
      "gams sn\n",
      "olloloy ther ciss\n",
      "nomese amin't med roped the thew your\n",
      "bromot wean b rehan\n",
      "that in'm yputkirn the bon't gacks thy allase\n",
      "bube besktill kean's me berithe s afnet af mond gomed gaas kind mo bute he tretlin' regige nitno p\n",
      "\n",
      "\n",
      "Epoch 7/10\n",
      "Loss:    \t 2.14402437210083\n",
      "Accuracy:\t 36.40581512451172\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems hos youn'med end hoiger buted fat lecpedin' ito wamp\n",
      "mi hiegin't with haw nouly staisey ond cuts holley the buckth\n",
      "sol yoy fena pake dike in wate war herye minor\n",
      "the hert me it i ciunk netsell the rooke op thifs a ciun\n",
      "i lumeay\n",
      "i'm nike my caond sulliys dote whilpry wiiby forgat if\n",
      "ane thes wipket's cowettcat a bhit\n",
      "how ther htoer ghat the gon'd woblend to fole oo thid ind yomr cand bute\n",
      "them nove andeny the canban't soyopret weiggar cay't got a now the al wey micha digm wherno\n",
      "neupe i woth you \n",
      "\n",
      "\n",
      "Epoch 8/10\n",
      "Loss:    \t 2.1013617515563965\n",
      "Accuracy:\t 37.368370056152344\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems to bust dape me sitoth and an hits iuckin\n",
      "tol the s bet i'll ho hafs bende baming you stoedn thied\n",
      "hacky iaane i now to i'w may notfouma\n",
      "i stande\n",
      "uidmi whe want hatsly wecans sokernd ulvery that my wen't hinl sall to cala you wispeythut in't ond ream nitha my icbucvin jurckind bo ginn hot buve butker \n",
      "smiken tale i mids it bute that if mith of the astoe the lotths the naldex i'ld i'm see bienye frople eno cout't thow wait the pnesil\n",
      "mf quck tle tang en my whet min't''s noped dous\n",
      "inf tul blaadja\n",
      "\n",
      "\n",
      "Epoch 9/10\n",
      "Loss:    \t 2.0674564838409424\n",
      "Accuracy:\t 38.337501525878906\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems itheving hot ild wheng the lowle hin'te\n",
      "ay mocke they the lack kek gite sto he wen2tous whin beld hepe\n",
      "i e me\n",
      "nother gra ning you lilpade traon on't stomch\n",
      "caz not tey cild seobed us\n",
      "in the llline romen i bare\n",
      "ave mid cou come a cullpic\n",
      "wher elate shap en wang tho stouthen't don't siolly me to fofp be hitoin as dis mike and\n",
      "chook willl\n",
      "beal ti'm deowe fre\n",
      "chige tes nime theand so thin for higga a nowl fun get chom avernot in a wat ond my way the bingand youp hemesmes and hit't my lumalis whas my\n",
      "\n",
      "\n",
      "Epoch 10/10\n",
      "Loss:    \t 2.0325331687927246\n",
      "Accuracy:\t 39.1075439453125\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems sos nighed to beas\n",
      "the foed ais tie hack bove capine this thing on bat erneme a no suod i't wimmron it acalloufl the gaz the lithers kerandis keevin' cow chan\n",
      "yous fued' the thiother he'l's weipfured son weedf on to toy nome\n",
      "showenle\n",
      "thay avely't that wecly gotcher called\n",
      "coucked at\n",
      "i'll tonind\n",
      "sronf comein't's as rimy my\n",
      "be asi't thaittt hom chibline noun me mokind fooking am bitin sell now mo coups wey wot ave mike and ie soul wo whacave buth whact\n",
      "bus selmy a cap caed iu in a moke they of tor\n"
     ]
    }
   ],
   "source": [
    "basicRNN = RNN(name = \"basic\")\n",
    "basicRNN.build(HIDDEN, VOCAB_SIZE, TIME_STEPS)\n",
    "basicRNN.train(tr_data, tr_labels, alphabet, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learnings\n",
    "Some of words slowly begin to make sense, but it is mostly gibberish. Instead of predicting letters, we should try to predict word by word!\n",
    "\n",
    "Have a look at **02-word-embedding** for that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60776, 20, 37)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
