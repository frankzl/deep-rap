{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    text = open(file_name, 'r').read()\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "class Alphabet:\n",
    "    def __init__(self, text):\n",
    "        from collections import Counter\n",
    "        self._count = Counter(list(text))\n",
    "        self._keys  = list(self._count.keys())\n",
    "        self._dict  = {}\n",
    "        for idx, key in enumerate(self._keys):\n",
    "            self._dict[key] = idx\n",
    "    \n",
    "    def get_count(self):\n",
    "        return self._count\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self._keys)\n",
    "    \n",
    "    def letter_to_index(self, letter):\n",
    "        return self._dict.get( letter, 'err' )\n",
    "    \n",
    "    def index_to_letter(self, index):\n",
    "        return self._keys[index]\n",
    "    \n",
    "    def one_hot(self, text):\n",
    "        encoded = []\n",
    "        for letter in text:\n",
    "            one_hot = [0] * self.get_size()\n",
    "            one_hot[self.letter_to_index(letter)] = 1\n",
    "            encoded.append(one_hot)\n",
    "        return np.array(encoded)\n",
    "    \n",
    "    def to_text(self, one_hots):\n",
    "        indices = np.argmax( one_hots, axis=1 ).tolist()\n",
    "        return \"\".join([self.index_to_letter(idx) for idx in indices])\n",
    "    \n",
    "    def indices_to_text(self, indices):\n",
    "        print(\"shape\")\n",
    "        print(indices.shape)\n",
    "        _indices = indices.tolist()\n",
    "        print(_indices)\n",
    "        return \"\".join([self.index_to_letter(idx) for idx in _indices])\n",
    "        \n",
    "    \n",
    "text = read_data(\"data/cleaned-rap-lyrics/clean2_pac_.txt\")\n",
    "alphabet = Alphabet(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet.get_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = np.array(alphabet.one_hot(text))\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"as real as it seems the american dream\\nain't nothing but another calculated schemes\\nto get us locked\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet.to_text(encoded)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(num_data, batch_size):\n",
    "    \"\"\" Yield batches with indices until epoch is over.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_data: int\n",
    "        The number of samples in the dataset.\n",
    "    batch_size: int\n",
    "        The batch size used using training.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    batch_ixs: np.array of ints with shape [batch_size,]\n",
    "        Yields arrays of indices of size of the batch size until the epoch is over.\n",
    "    \"\"\"\n",
    "    \n",
    "    # data_ixs = np.random.permutation(np.arange(num_data))\n",
    "    data_ixs = np.arange(num_data)\n",
    "    ix = 0\n",
    "    while ix + batch_size < num_data:\n",
    "        batch_ixs = data_ixs[ix:ix+batch_size]\n",
    "        ix += batch_size\n",
    "        yield batch_ixs\n",
    "\n",
    "def sample(predicted, temperature=0.1):\n",
    "    '''\n",
    "     helper function to sample an index from a probability array\n",
    "    '''\n",
    "    exp_predicted = np.exp(predicted/temperature)\n",
    "    predicted = exp_predicted / np.sum(exp_predicted)\n",
    "    probabilities = np.random.multinomial(1, predicted, 1)\n",
    "    return probabilities\n",
    "        \n",
    "class RNN:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "    def build(self, hidden_layer_size, vocab_size, time_steps, l2_reg=0.0):\n",
    "        self.time_steps = time_steps\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape=[None, time_steps, vocab_size], name=\"data\")\n",
    "        self.Y = tf.placeholder(tf.int16, shape=[None, vocab_size], name=\"labels\")\n",
    "        \n",
    "        _X = tf.transpose(self.X, [1, 0, 2])\n",
    "        _X = tf.reshape(_X, [-1, vocab_size])\n",
    "        _X = tf.split(_X, time_steps, 0)\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "            self.rnn_cell   = tf.nn.rnn_cell.LSTMCell(hidden_layer_size)\n",
    "            \n",
    "            self.outputs, _ = tf.contrib.rnn.static_rnn(self.rnn_cell, _X, dtype=tf.float32)\n",
    "            \n",
    "            W_out = tf.Variable(tf.truncated_normal([hidden_layer_size, vocab_size], \n",
    "                                                 mean=0, stddev=.01))\n",
    "            b_out = tf.Variable(tf.truncated_normal([vocab_size],\n",
    "                                                mean=0, stddev=.01))\n",
    "            \n",
    "            self.weights.append(W_out)\n",
    "            self.biases.append(b_out)\n",
    "            \n",
    "            self.last_rnn_output = self.outputs[-1]\n",
    "            self.final_output    = self.last_rnn_output @ W_out + b_out\n",
    "            \n",
    "            self.softmax = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.final_output,\n",
    "                                                                labels=self.Y)\n",
    "            self.cross_entropy_loss = tf.reduce_mean(self.softmax)\n",
    "            \n",
    "            self.loss = self.cross_entropy_loss\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer()\n",
    "            self.train_step= self.optimizer.minimize(self.loss)\n",
    "            \n",
    "            self.correct_prediction = tf.equal(tf.argmax(self.Y,1), tf.argmax(self.final_output, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))*100\n",
    "    \n",
    "    def train(self, train_data, train_labels, alphabet, epochs=20, batch_size=128):\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        \n",
    "        self.session = tf.Session()\n",
    "        session = self.session\n",
    "        \n",
    "        with session.as_default():\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            tr_loss, tr_acc = session.run([self.loss, self.accuracy],\n",
    "                                          feed_dict={self.X: train_data,\n",
    "                                                     self.Y: train_labels})\n",
    "            train_losses.append(tr_loss)\n",
    "            train_accs.append(tr_acc)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                \n",
    "                if(epoch + 1) % 1 == 0:\n",
    "                    print(f\"\\n\\nEpoch {epoch + 1}/{epochs}\")\n",
    "                    print(f\"Loss:    \\t {tr_loss}\")\n",
    "                    print(f\"Accuracy:\\t {tr_acc}\")\n",
    "                \n",
    "                for batch_ixs in batch_data(len(train_data), batch_size):\n",
    "                    _ = session.run(self.train_step,\n",
    "                                   feed_dict={\n",
    "                                       self.X: train_data[batch_ixs],\n",
    "                                       self.Y: train_labels[batch_ixs],\n",
    "                                   })\n",
    "                tr_loss, tr_acc = session.run([self.loss, self.accuracy],\n",
    "                                               feed_dict={self.X: train_data,\n",
    "                                                          self.Y: train_labels\n",
    "                                                         })\n",
    "                train_losses.append(tr_loss)\n",
    "                train_accs.append(tr_acc)\n",
    "                \n",
    "                #get on of training set as seed\n",
    "                seed = train_data[:1:]\n",
    "        \n",
    "                #to print the seed 40 characters\n",
    "                seed_chars = ''\n",
    "                for each in seed[0]:\n",
    "                    seed_chars += alphabet._keys[np.where(each == max(each))[0][0]]\n",
    "                print (\"Seed:\" + seed_chars)\n",
    "        \n",
    "                #predict next 500 characters\n",
    "                for i in range(500):\n",
    "                    if i > 0:\n",
    "                        remove_fist_char = seed[:,1:,:]\n",
    "                        seed = np.append(remove_fist_char, np.reshape(probabilities, [1, 1, self.vocab_size]), axis=1)\n",
    "                        \n",
    "                    predicted = session.run([self.final_output], feed_dict = {self.X:seed})\n",
    "                    predicted = np.asarray(predicted[0]).astype('float64')[0]\n",
    "                    probabilities = sample(predicted)\n",
    "                    predicted_chars = alphabet._keys[np.argmax(probabilities)]\n",
    "                    seed_chars += predicted_chars\n",
    "                print ('Result:'+ seed_chars)\n",
    "        \n",
    "        self.hist = {\n",
    "            'train_losses': np.array(train_losses),\n",
    "            'train_accuracy': np.array(train_accs)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_data('data/cleaned-rap-lyrics/clean2_pac_.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "\n",
    "HIDDEN = 128\n",
    "VOCAB_SIZE = 31\n",
    "TIME_STEPS = 20\n",
    "EPOCHS = 10\n",
    "\n",
    "def making_one_hot(text, alphabet):\n",
    "    '''\n",
    "    '''\n",
    "    unique_chars = alphabet._keys\n",
    "    len_unique_chars = len(unique_chars)\n",
    "\n",
    "    input_chars = []\n",
    "    output_char = []\n",
    "    for i in range(0, len(text) - TIME_STEPS, step):\n",
    "        input_chars.append(text[i:i+TIME_STEPS])\n",
    "        output_char.append(text[i+TIME_STEPS])\n",
    "    train_data = np.zeros((len(input_chars), TIME_STEPS, len_unique_chars))\n",
    "    target_data = np.zeros((len(input_chars), len_unique_chars))\n",
    "    for i , each in enumerate(input_chars):\n",
    "        for j, char in enumerate(each):\n",
    "            train_data[i, j, unique_chars.index(char)] = 1\n",
    "        target_data[i, unique_chars.index(output_char[i])] = 1\n",
    "    return train_data, target_data, unique_chars, len_unique_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data, tr_labels, unique_chars, len_unique = making_one_hot(text, alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "inputs must be a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2eac84d7df68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbasicRNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"basic\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbasicRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHIDDEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTIME_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbasicRNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphabet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-ebc1808f438d>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, hidden_layer_size, vocab_size, time_steps, l2_reg)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             W_out = tf.Variable(tf.truncated_normal([hidden_layer_size, vocab_size], \n",
      "\u001b[0;32m~/.envs/env36-ml/.venv/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mstatic_rnn\u001b[0;34m(cell, inputs, initial_state, dtype, sequence_length, scope)\u001b[0m\n\u001b[1;32m   1277\u001b[0m   \u001b[0mrnn_cell_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_like_rnncell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cell\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1279\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inputs must be a sequence\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1280\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inputs must not be empty\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: inputs must be a sequence"
     ]
    }
   ],
   "source": [
    "basicRNN = RNN(name = \"basic\")\n",
    "basicRNN.build(HIDDEN, VOCAB_SIZE, TIME_STEPS)\n",
    "basicRNN.train(tr_data, tr_labels, alphabet, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
