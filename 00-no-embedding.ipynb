{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting letter by letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    text = open(file_name, 'r').read()\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "class Alphabet:\n",
    "    def __init__(self, text):\n",
    "        from collections import Counter\n",
    "        self._count = Counter(list(text))\n",
    "        self._keys  = list(self._count.keys())\n",
    "        self._dict  = {}\n",
    "        for idx, key in enumerate(self._keys):\n",
    "            self._dict[key] = idx\n",
    "    \n",
    "    def get_count(self):\n",
    "        return self._count\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self._keys)\n",
    "    \n",
    "    def letter_to_index(self, letter):\n",
    "        return self._dict.get( letter, 'err' )\n",
    "    \n",
    "    def index_to_letter(self, index):\n",
    "        return self._keys[index]\n",
    "    \n",
    "    def one_hot(self, text):\n",
    "        encoded = []\n",
    "        for letter in text:\n",
    "            one_hot = [0] * self.get_size()\n",
    "            one_hot[self.letter_to_index(letter)] = 1\n",
    "            encoded.append(one_hot)\n",
    "        return np.array(encoded)\n",
    "    \n",
    "    def to_text(self, one_hots):\n",
    "        indices = np.argmax( one_hots, axis=1 ).tolist()\n",
    "        return \"\".join([self.index_to_letter(idx) for idx in indices])\n",
    "    \n",
    "    def indices_to_text(self, indices):\n",
    "        print(\"shape\")\n",
    "        print(indices.shape)\n",
    "        _indices = indices.tolist()\n",
    "        print(_indices)\n",
    "        return \"\".join([self.index_to_letter(idx) for idx in _indices])\n",
    "        \n",
    "    \n",
    "text = read_data(\"data/cleaned-rap-lyrics/clean2_pac_.txt\")\n",
    "alphabet = Alphabet(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unique characters: 36\n"
     ]
    }
   ],
   "source": [
    "print(f\"# unique characters: {alphabet.get_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = np.array(alphabet.one_hot(text))\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"as real as it seems the american dream\\nain't nothing but another calculated schemes\\nto get us locked\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet.to_text(encoded)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(num_data, batch_size):\n",
    "    \"\"\" Yield batches with indices until epoch is over.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_data: int\n",
    "        The number of samples in the dataset.\n",
    "    batch_size: int\n",
    "        The batch size used using training.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    batch_ixs: np.array of ints with shape [batch_size,]\n",
    "        Yields arrays of indices of size of the batch size until the epoch is over.\n",
    "    \"\"\"\n",
    "    \n",
    "    # data_ixs = np.random.permutation(np.arange(num_data))\n",
    "    data_ixs = np.arange(num_data)\n",
    "    ix = 0\n",
    "    while ix + batch_size < num_data:\n",
    "        batch_ixs = data_ixs[ix:ix+batch_size]\n",
    "        ix += batch_size\n",
    "        yield batch_ixs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Classification\n",
    "\n",
    "What we are building in this first instance is a 36-class classifier. Based on the previous characters, our model will predict the next upcoming character!\n",
    "\n",
    "#### Network Architecture:\n",
    "1. **RNN cell**: LSTM with hidden_layer_size\n",
    "2. **linear output layer**: maps to scores for 36 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(predicted, temperature=0.9):\n",
    "    '''\n",
    "     helper function to sample an index from a probability array\n",
    "     our model will output scores for each class\n",
    "     we normalize those outputs and create a probability distribution out of them to sample from\n",
    "    '''\n",
    "    exp_predicted = np.exp(predicted/temperature)\n",
    "    predicted = exp_predicted / np.sum(exp_predicted)\n",
    "    probabilities = np.random.multinomial(1, predicted, 1)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "    def build(self, hidden_layer_size, vocab_size, time_steps, l2_reg=0.0):\n",
    "        self.time_steps = time_steps\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape=[None, time_steps, vocab_size], name=\"data\")\n",
    "        self.Y = tf.placeholder(tf.int16, shape=[None, vocab_size], name=\"labels\")\n",
    "        \n",
    "        _X = tf.transpose(self.X, [1, 0, 2])\n",
    "        _X = tf.reshape(_X, [-1, vocab_size])\n",
    "        _X = tf.split(_X, time_steps, 0)\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            # 1x RNN LSTM Cell\n",
    "            self.rnn_cell   = tf.nn.rnn_cell.LSTMCell(hidden_layer_size)\n",
    "            \n",
    "            self.outputs, _ = tf.contrib.rnn.static_rnn(self.rnn_cell, _X, dtype=tf.float32)\n",
    "            \n",
    "            # 1x linear output layer\n",
    "            W_out = tf.Variable(tf.truncated_normal([hidden_layer_size, vocab_size], \n",
    "                                                 mean=0, stddev=.01))\n",
    "            b_out = tf.Variable(tf.truncated_normal([vocab_size],\n",
    "                                                mean=0, stddev=.01))\n",
    "            self.weights.append(W_out)\n",
    "            self.biases.append(b_out)\n",
    "            \n",
    "            self.last_rnn_output = self.outputs[-1]\n",
    "            self.final_output    = self.last_rnn_output @ W_out + b_out\n",
    "            \n",
    "            # softmax cross entropy as our loss function (between 36 classes)\n",
    "            self.softmax = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.final_output,\n",
    "                                                                labels=self.Y)\n",
    "            self.cross_entropy_loss = tf.reduce_mean(self.softmax)\n",
    "            \n",
    "            self.loss = self.cross_entropy_loss\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer()\n",
    "            self.train_step= self.optimizer.minimize(self.loss)\n",
    "            \n",
    "            self.correct_prediction = tf.equal(tf.argmax(self.Y,1), tf.argmax(self.final_output, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))*100\n",
    "    \n",
    "    def train(self, train_data, train_labels, alphabet, epochs=20, batch_size=128):\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        \n",
    "        self.session = tf.Session()\n",
    "        session = self.session\n",
    "        \n",
    "        with session.as_default():\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            tr_loss, tr_acc = session.run([self.loss, self.accuracy],\n",
    "                                          feed_dict={self.X: train_data,\n",
    "                                                     self.Y: train_labels})\n",
    "            train_losses.append(tr_loss)\n",
    "            train_accs.append(tr_acc)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                \n",
    "                if(epoch + 1) % 1 == 0:\n",
    "                    print(f\"\\n\\nEpoch {epoch + 1}/{epochs}\")\n",
    "                    print(f\"Loss:    \\t {tr_loss}\")\n",
    "                    print(f\"Accuracy:\\t {tr_acc}\")\n",
    "                \n",
    "                for batch_ixs in batch_data(len(train_data), batch_size):\n",
    "                    _ = session.run(self.train_step,\n",
    "                                   feed_dict={\n",
    "                                       self.X: train_data[batch_ixs],\n",
    "                                       self.Y: train_labels[batch_ixs],\n",
    "                                   })\n",
    "                tr_loss, tr_acc = session.run([self.loss, self.accuracy],\n",
    "                                               feed_dict={self.X: train_data,\n",
    "                                                          self.Y: train_labels\n",
    "                                                         })\n",
    "                train_losses.append(tr_loss)\n",
    "                train_accs.append(tr_acc)\n",
    "                \n",
    "                #get on of training set as seed\n",
    "                seed = train_data[:1:]\n",
    "        \n",
    "                #to print the seed 40 characters\n",
    "                seed_chars = ''\n",
    "                for each in seed[0]:\n",
    "                    seed_chars += alphabet._keys[np.where(each == max(each))[0][0]]\n",
    "                print (\"Seed:\" + seed_chars)\n",
    "        \n",
    "                #predict next 500 characters\n",
    "                for i in range(500):\n",
    "                    if i > 0:\n",
    "                        remove_fist_char = seed[:,1:,:]\n",
    "                        seed = np.append(remove_fist_char, np.reshape(probabilities, [1, 1, self.vocab_size]), axis=1)\n",
    "                        \n",
    "                    predicted = session.run([self.final_output], feed_dict = {self.X:seed})\n",
    "                    predicted = np.asarray(predicted[0]).astype('float64')[0]\n",
    "                    probabilities = sample(predicted)\n",
    "                    predicted_chars = alphabet._keys[np.argmax(probabilities)]\n",
    "                    seed_chars += predicted_chars\n",
    "                print ('Result:'+ seed_chars)\n",
    "        \n",
    "        self.hist = {\n",
    "            'train_losses': np.array(train_losses),\n",
    "            'train_accuracy': np.array(train_accs)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_data('data/cleaned-rap-lyrics/clean2_pac_.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 1\n",
    "\n",
    "HIDDEN = 128\n",
    "VOCAB_SIZE = 36\n",
    "TIME_STEPS = 20\n",
    "EPOCHS = 10\n",
    "\n",
    "def making_one_hot(text, alphabet):\n",
    "    '''\n",
    "    '''\n",
    "    unique_chars = alphabet._keys\n",
    "    len_unique_chars = len(unique_chars)\n",
    "\n",
    "    input_chars = []\n",
    "    output_char = []\n",
    "    for i in range(0, len(text) - TIME_STEPS, step):\n",
    "        input_chars.append(text[i:i+TIME_STEPS])\n",
    "        output_char.append(text[i+TIME_STEPS])\n",
    "    train_data = np.zeros((len(input_chars), TIME_STEPS, len_unique_chars))\n",
    "    target_data = np.zeros((len(input_chars), len_unique_chars))\n",
    "    for i , each in enumerate(input_chars):\n",
    "        for j, char in enumerate(each):\n",
    "            train_data[i, j, unique_chars.index(char)] = 1\n",
    "        target_data[i, unique_chars.index(output_char[i])] = 1\n",
    "    return train_data, target_data, unique_chars, len_unique_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data, tr_labels, unique_chars, len_unique = making_one_hot(text, alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39985, 20, 36)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 1/10\n",
      "Loss:    \t 3.58311128616333\n",
      "Accuracy:\t 1.9007127285003662\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems  pe l  etmos oil ie lrvbhdaoooha lhn eigagem ehhh k'nted  fyumrl o ianishn\n",
      " le  nbmaeyvu' \n",
      "yk  o  thahtt oolslttr  v pgsnwt phoi otatolanee dh oan  i rdloh asaaid  ahyjmru 'r wih  m r nc tgstd kh  c  rimostpcwak \n",
      "e  n r  na f t hottwoaaantaljneaoublfwafdr n  t 9teg o salcbw ledu o mch ck d  ureetnowcigodr feddtn   fiwotmt'ra r  meoso fnd feewni deleeaniw ihn  igsyii eit in hoe stiiel  i oyrso erueiy taaf sh pihee  ts a  ttesoiouwosa arhssierita  hee eagod t p yrmgm ls   gnre s eo' rtettip nhagtl\n",
      "\n",
      "\n",
      "Epoch 2/10\n",
      "Loss:    \t 2.9071054458618164\n",
      "Accuracy:\t 17.976741790771484\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems nacwii wftoek ck  ae tamidehhetoaloal the the dayuai sanw bhsgei icutve l\n",
      "goweutuss bhoy mls tbn wo \n",
      "aobrttld  he2i mano uofh eee ia s e guwmen elt wet' ehisit foth bon nt ny t'at ahe'i thy'w npd\n",
      "\n",
      "mas dhor\n",
      "smdens st ky' chdhe ga 'ss eea bet\n",
      "cvnd set sn iad ce shaeseboheg spe gechoa  n weycaamydl nonn hoini wehonktny hhr ten nle hov sotuth a e bot ne  eolhge  id 'dyy won' nat strhe ne f' ntteugacnhstntacege ie mee me\n",
      "bam mhrehon chlk c\n",
      "soagws o in\n",
      "ee bmo n\n",
      "ph'wsytl ud hoshaet jelalu rida hoee iti\n",
      "\n",
      "\n",
      "Epoch 3/10\n",
      "Loss:    \t 2.657147169113159\n",
      "Accuracy:\t 25.7896728515625\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems anrr be them\n",
      "giut iia at areiee i taan nou' kt aasatas sagj fure h\n",
      "ron' wou' aam sish ble name  poat ihe gacoon chon wherdhocgbeuthicg uuthe mad'e wecaningy it bunsin du bame ch\n",
      "leuw apbsox ws hinls topri birh'the apyred papu ip\n",
      "mewninl dhiih ut ics\n",
      "ternflen nheg in kan ge kos sh sets ge ire \n",
      "eosiir stlend thet yetoin conat\n",
      "shsthetk y cuun'it yoeit\n",
      "e dheece n' butgelc the bmawns nas\n",
      "uhari to tua u'l to te chchpo she dos ne mp tere yu aseat tu nr themed grimn goat  med irgle sostitn couto taun pa\n",
      "\n",
      "\n",
      "Epoch 4/10\n",
      "Loss:    \t 2.4924118518829346\n",
      "Accuracy:\t 28.210580825805664\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems bithe porrghey  iyas that\n",
      "in thes mium th\n",
      "mer wwrowe beoteobe ctiikid auvo toowyon y poum tout\n",
      "gewgot\n",
      "inesc and\n",
      "iot\n",
      "upe and mac urerandthinhy th thuo tgan ju wabkisse le\n",
      "bh in shand 'age cortazrthe rimk at mits or mna whansjy tiy goshe keyde yhe bout e indam\n",
      "pak the a hu'ke pt woul\n",
      "let cousl ney\n",
      "nad frasrecd nol tue bar chounn ane irert muond acrekanethe itd soonel glo houd ingos\n",
      "\n",
      "angpangthe lone ahe wonll 5'lee  nofas thote waor thrt ulit s causs irucly cathes\n",
      "m anink he thebthe ghul' os gad\n",
      "th\n",
      "\n",
      "\n",
      "Epoch 5/10\n",
      "Loss:    \t 2.411022186279297\n",
      "Accuracy:\t 29.46605110168457\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems some pathir ayoo micleurh ed aspipge pane bed mrlkspan wit\n",
      "y wothe\n",
      "hor thwange fit shist pot tha 'lony theofarm an thejt athe nithe nothe nat death s ang\n",
      "hes heatew ponit\n",
      "jospsay y he thasksthet\n",
      "wonpind mit i' fers yhetchse 2tis he youkicr the alit thin't fathe jo\n",
      "ganlplek thi\n",
      "t anlet bunca ont they ilkis' ithet wyank eog ally coa gelt brra ii iflses conslr than notath\n",
      "and uod fean sidee mee yherthy whuug tef jte'i yie inan ate homte na mest may thy eus ama mfthelgic' wau ghe sli ind faops\n",
      "fh as\n",
      "\n",
      "\n",
      "Epoch 6/10\n",
      "Loss:    \t 2.3568737506866455\n",
      "Accuracy:\t 30.531448364257812\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems ine a byom dame be cian the cas\n",
      "towhil s me cekg wis dof salie\n",
      "goley fhind\n",
      "deut worpen hoar mrin theot bades le sban'ngathin'r phet dol't wots id the tathe in\n",
      "bes reacn me ahis jlig eisl\n",
      "in i she ger tiu\n",
      "lacf\n",
      "whouke'id oungley matheeee co bun thiswine bouwiy me go gat my m roc'me couggitre kof laves herr cacg moukis' dii't\n",
      "be isee bale pescken bopgis k asge siceipt at\n",
      "soutta thithe caad thanf tome heuby routheg yout th nee wous\n",
      "an slos'ttsin\n",
      "sifon an bink\n",
      "bun ya ord mighen ay\n",
      "rot i ba than theas\n",
      "\n",
      "\n",
      "Epoch 7/10\n",
      "Loss:    \t 2.3101963996887207\n",
      "Accuracy:\t 31.044139862060547\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems burlonk thean theateu haugat it ga bubit\n",
      "its angtrifs ctos a deren yor ta chege edipjr anti mome hor in who hee that to pon angich rillin' guve whot i'e nit  nof at weas in thamz\n",
      "1erano bucgesle inn whacs y an wayhea stat me keat esmio tii corothre ipe rothe iud io ceak ind is that wo and d oaw tuthe pis wtalf met\n",
      " alk kais andin thy cainj thye ae whelcaren ine cuad\n",
      "sous craat\n",
      "she forla genifid dougat\n",
      "joty i geatind mane chont iad titt\n",
      "ut\n",
      "s caee aps afkin se\n",
      "th thon ho wer mutt entho keovecd\n",
      "she\n",
      "\n",
      "\n",
      "Epoch 8/10\n",
      "Loss:    \t 2.268301486968994\n",
      "Accuracy:\t 31.809429168701172\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems yous\n",
      "besu hoc tame he nous ind a nravet i mon thind o they a bulgr trog was me her wisim tey ked pad theat hilk joc a bo kearelnt gagt\n",
      "hi kepplko mnd inc youkl he tt want\n",
      "yome ind disinr moted soreat bald wor ni clange bes ma heap 2okly i thaok\n",
      "you cough bwachind pume me igin\n",
      "itz rest the ther buct ghas dast werthos at undetrsind muthan ricke\n",
      "coud an now tock ma tant the lethy asle you i' tiin ar ther\n",
      "oukl not ge socla pand thel\n",
      "ther bees jo re dyoud beaws and peik show rean'g hith then's\n",
      "sit\n",
      "in\n",
      "\n",
      "\n",
      "Epoch 9/10\n",
      "Loss:    \t 2.231109619140625\n",
      "Accuracy:\t 32.93735122680664\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems theay tom thersthine buckeand blath on coulwi ganot\n",
      "i\n",
      "ind ba cacredins af yonpedhithe gackee nou wause fullles angeand s ette ne lyre bum the at the ceacke gaingit i't ead booggi't hinn hank the wanky sige coums bungi heas yau dase a domlt oren yiem ii in'the \n",
      "helan' th ow hessen ke mothe sastud walcin le 2ethe whin' that wa chy ar thes line had on a cone callin' wath\n",
      "thars wethy of dikl gatke thei't hi  hoaseed methe the lant i'm that i'm merean't an tha me be youcher coucthe ker thed an the pr\n",
      "\n",
      "\n",
      "Epoch 10/10\n",
      "Loss:    \t 2.197934150695801\n",
      "Accuracy:\t 33.69763946533203\n",
      "Seed:as real as it seems \n",
      "Result:as real as it seems thotizn\n",
      "afino ald you lackin' to in's a tou tos couthe saro me whes ge the hon caf stack angist hrowin'ss rita caun's bmothe\n",
      "thothes ow the trey oucke you thang rik riat tho a watnet thin her bftald\n",
      "ge thare werpidere wai't seale ni hect my sathin wher a thercang buele ghink donk hiveed a thet mi ther gickinn the\n",
      "beere mrorllached the me faule ind hathe micou the mening seey dorsty\n",
      "e anne thtane she\n",
      "ta wiln is\n",
      "they\n",
      "so tou bongawi''t poly michith\n",
      "bopernet ald mlaed bother\n",
      "casger pes\n",
      "i muves ur al\n"
     ]
    }
   ],
   "source": [
    "basicRNN = RNN(name = \"basic\")\n",
    "basicRNN.build(HIDDEN, VOCAB_SIZE, TIME_STEPS)\n",
    "basicRNN.train(tr_data, tr_labels, alphabet, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learnings\n",
    "Some of words slowly begin to make sense, but it is mostly gibberish. Instead of predicting letters, we should try to predict word by word!\n",
    "\n",
    "Have a look at **02-word-embedding** for that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
